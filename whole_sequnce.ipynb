{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 기본 설정"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "아래에 사용한 모듈들은 추가 및 제거가 가능"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import monai\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torchvision\r\n",
    "import torchvision.transforms as transforms\r\n",
    "import torchvision.models as models\r\n",
    "import torch.optim as optim\r\n",
    "\r\n",
    "# for image\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "import os\r\n",
    "import glob\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "\r\n",
    "from monai.apps import download_and_extract\r\n",
    "from monai.config import print_config\r\n",
    "from monai.data import decollate_batch\r\n",
    "from monai.metrics import ROCAUCMetric\r\n",
    "from monai.networks.nets import DenseNet121\r\n",
    "from monai.transforms import (\r\n",
    "    Activations,\r\n",
    "    AddChannel,\r\n",
    "    AsDiscrete,\r\n",
    "    Compose,\r\n",
    "    LoadImage,\r\n",
    "    RandFlip,\r\n",
    "    RandRotate,\r\n",
    "    RandZoom,\r\n",
    "    ScaleIntensity,\r\n",
    "    EnsureType,\r\n",
    ")\r\n",
    "from monai.utils import set_determinism"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "dir = 'D:/python_study/'\r\n",
    "# 디렉토리 설정으로 가장 상위 디렉토리를 권장"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "image = sorted(glob.glob(os.path.join(dir, 'image', '*.png')))\r\n",
    "mask = sorted(glob.glob(os.path.join(dir, 'filled_label', '*.png')))\r\n",
    "\r\n",
    "# glob.glob를 이용하여 좀 더 간단하게 접근\r\n",
    "# 차후 이미지의 개수를 기준으로 폴더를 나눔"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "loader = monai.transforms.LoadImage(monai.data.PILReader(converter=lambda image: image.convert(\"RGBA\")))\r\n",
    "\r\n",
    "# 모나이에서 지원하는 이미지 로더를 사용하며, RGBA 4 채널을 사용 "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Used Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "아래에서 기능의 형태로 사용한 함수들을 한곳에 정리\r\n",
    "\r\n",
    "폴더 생성, 폴더 하위의 파일과 그에 관련한 레이블 생성 함수\r\n",
    "\r\n",
    "폴더 생성 : os.makedirs를 이용하여 원하는 폴더(디렉토리)가 없는 경우 생성\r\n",
    "\r\n",
    "파일과 레이블 생성 : 폴더 번호를 입력 받아 그 하위에 있는 파일과 파일에 해당하는 레이블을 생성"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def create(dir):\r\n",
    "    try:\r\n",
    "        if not os.path.exists(dir):\r\n",
    "            os.makedirs(dir)\r\n",
    "    except:\r\n",
    "        print('Error : creating directory. ' + dir) # 오류가 발생하는 경우 메세지 출력"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def files_and_labels(folder_type):\r\n",
    "\r\n",
    "    files = []\r\n",
    "    labels = []\r\n",
    "\r\n",
    "    for i in folder_type:\r\n",
    "\r\n",
    "        files += glob.glob(os.path.join(dir, 'patches/*', f'img{i+1:02d}', '*.png')) # 폴더(디렉토리) 번호에 해당하는 폴더 하위 파일들을 리스트의 형태로 반환  \r\n",
    "\r\n",
    "    for file in files: # 파일들 경로에 'cancer'란 단어가 있는 경우 0, 없는 경우 1을 지정\r\n",
    "        if 'cancer' in file:\r\n",
    "            labels.append(0)\r\n",
    "        else:\r\n",
    "            labels.append(1)\r\n",
    "\r\n",
    "    return files, labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "patches = dir + '/patches'\r\n",
    "d_cancer = patches + '/cancer'\r\n",
    "d_normal = patches + '/normal'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Patch generate sequence"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "patch_size = (200,200)\r\n",
    "\r\n",
    "# 패치 크기, 현재 200 * 200으로 설정"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "normal cells"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "create(patches)\r\n",
    "create(d_cancer)\r\n",
    "create(d_normal)\r\n",
    "\r\n",
    "pat_num = 1000\r\n",
    "\r\n",
    "for k in range(len(image)):\r\n",
    "    \r\n",
    "    print(f'Processing image no.{k+1}')\r\n",
    "\r\n",
    "    count = 0\r\n",
    "\r\n",
    "    cancer_im = (f'{d_cancer}' + f'/img{k+1:02d}')\r\n",
    "\r\n",
    "    create(cancer_im)\r\n",
    "\r\n",
    "    img_file = image[k]\r\n",
    "    mask_file = mask[k]\r\n",
    "\r\n",
    "    image_data, image_meta = loader(img_file)\r\n",
    "    mask_data, mask_meta = loader(mask_file)\r\n",
    "\r\n",
    "    mtemp_position = []\r\n",
    "    m_position = []\r\n",
    "    \r\n",
    "    x, y, z = np.where(mask_data/255 == 1)\r\n",
    "\r\n",
    "    for m_pos in zip(x, y):\r\n",
    "        lm_pos = list(m_pos)\r\n",
    "        mtemp_position.append(lm_pos)\r\n",
    "\r\n",
    "    for i in range(0, len(mtemp_position), 3):\r\n",
    "        if mtemp_position[i][0]>=100 and mtemp_position[i][1]>= 100:\r\n",
    "            m_position.append(mtemp_position[i])\r\n",
    "\r\n",
    "    m_sample = random.sample(m_position, pat_num)\r\n",
    "\r\n",
    "    for n in range(len(m_sample)):\r\n",
    "        cancer = image_data[m_sample[n][0]-100:m_sample[n][0]+100, m_sample[n][1]-100:m_sample[n][1]+100, :] / 255\r\n",
    "        plt.imsave(f'D:/python_study/patches/cancer/img{k+1:02d}/Image{k+1:02d}{n+1:04d}_abnormal.png', cancer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pat_num = 1000\r\n",
    "\r\n",
    "for k in range(len(image)):\r\n",
    "    \r\n",
    "    print(f'Processing image no.{k+1}')\r\n",
    "\r\n",
    "    count = 0\r\n",
    "\r\n",
    "    normal_im = (f'{d_normal}' + f'/img{k+1:02d}')\r\n",
    "\r\n",
    "    create(normal_im)\r\n",
    "\r\n",
    "    itemp_position = []\r\n",
    "    i_position = []\r\n",
    "\r\n",
    "    img_file = image[k]\r\n",
    "    mask_file = mask[k]\r\n",
    "\r\n",
    "    image_data, image_meta = loader(img_file)\r\n",
    "    mask_data, mask_meta = loader(mask_file)\r\n",
    "      \r\n",
    "    q, w, e = np.where((image_data/255 != 0) & (mask_data/255 == 0))\r\n",
    "\r\n",
    "    for i_pos in zip(q, w):\r\n",
    "        li_pos = list(i_pos)\r\n",
    "        itemp_position.append(li_pos)\r\n",
    "\r\n",
    "    for j in range(0, len(itemp_position), 3):\r\n",
    "        if itemp_position[j][1]>=200:\r\n",
    "            i_position.append(itemp_position[j])\r\n",
    "\r\n",
    "    i_sample = random.sample(i_position, pat_num*5)\r\n",
    "\r\n",
    "    for l in range(len(i_sample)):\r\n",
    "        \r\n",
    "        normal = image_data[i_sample[l][0]-200:i_sample[l][0], i_sample[l][1]-200:i_sample[l][1], :] / 255\r\n",
    "        normal_mask = mask_data[i_sample[l][0]-200:i_sample[l][0], i_sample[l][1]-200:i_sample[l][1], :] / 255\r\n",
    "        \r\n",
    "        if sum(sum(sum(normal_mask == 0))) >= normal_mask.size*0.8:\r\n",
    "            plt.imsave(f'D:/python_study/patches/normal/img{k+1:02d}/Image{k+1:02d}{l+1:04d}_normal.png', normal)\r\n",
    "            count += 1\r\n",
    "\r\n",
    "        if count == pat_num:\r\n",
    "            break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset seqence"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# seed = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# train : valid : test === 6.5 : 2.5 : 1 의 비율로 시작 하였으나 변경 가능\r\n",
    " \r\n",
    "valid_rat = 0.25\r\n",
    "test_rat = 0.1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "sample_num = len(image) # sample 수를 기준으로 데이터 비율 조정\r\n",
    "\r\n",
    "train_c = int(sample_num * (1 - (valid_rat + test_rat)))\r\n",
    "valid_c = int(sample_num * valid_rat)\r\n",
    "test_c = int(sample_num * test_rat)\r\n",
    "\r\n",
    "train_c, valid_c, test_c"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(13, 5, 2)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "folder_num = list(range(sample_num))\r\n",
    "\r\n",
    "# 단순 앞에서부터 파일 번호 부여"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "folder_num"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "train_folder = folder_num[:train_c]\r\n",
    "valid_folder = folder_num[train_c : train_c + valid_c]\r\n",
    "test_folder = folder_num[-test_c:]\r\n",
    "\r\n",
    "print(train_folder, '\\n', valid_folder, '\\n', test_folder)\r\n",
    "\r\n",
    "# 생성된 폴더 번호를 데이터 비율에 맞게 분배"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] \n",
      " [13, 14, 15, 16, 17] \n",
      " [18, 19]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class_names = sorted(x for x in os.listdir(patches)\r\n",
    "                     if os.path.isdir(os.path.join(patches, x)))\r\n",
    "\r\n",
    "num_class = len(class_names)\r\n",
    "\r\n",
    "# 클래스(레이블) 이름 지정\r\n",
    "# 0 for cancer, 1 for normal"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "train_x, train_y = files_and_labels(train_folder)\r\n",
    "\r\n",
    "val_x, val_y = files_and_labels(valid_folder)\r\n",
    "\r\n",
    "test_x, test_y = files_and_labels(valid_folder)\r\n",
    "\r\n",
    "# 데이터 분류\r\n",
    "# 위에 정의한 함수를 통해 하위 파일과, 파일에 해당하는 레이블을 가져옴"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "이미지 크기 확인 코드"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "for item in train_x:\r\n",
    "    if loader(item)[0].shape != (200,200,4):\r\n",
    "        print(item)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "for item in val_x:\r\n",
    "    if loader(item)[0].shape != (200,200,4):\r\n",
    "        print(item)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "for item in test_x:\r\n",
    "    if loader(item)[0].shape != (200,200,4):\r\n",
    "        print(item)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training sequence"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "기존에 사용하던 모델 학습 기법을 사용\r\n",
    "\r\n",
    "학습 모델 및 여러가지를 변경하여 사용할수 있음"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "train_transforms = Compose(\r\n",
    "    [\r\n",
    "        LoadImage(image_only=True),\r\n",
    "        # AddChannel(),\r\n",
    "        ScaleIntensity(),\r\n",
    "        RandRotate(range_x=np.pi / 12, prob=0.5, keep_size=True),\r\n",
    "        RandFlip(spatial_axis=0, prob=0.5), \r\n",
    "        RandZoom(min_zoom=0.9, max_zoom=1.1, prob=0.5), \r\n",
    "        EnsureType(),\r\n",
    "    ]\r\n",
    ")\r\n",
    "\r\n",
    "val_transforms = Compose(\r\n",
    "    [LoadImage(image_only=True), ScaleIntensity(), EnsureType()])\r\n",
    "\r\n",
    "y_pred_trans = Compose([EnsureType(), Activations(softmax=True)])\r\n",
    "y_trans = Compose([EnsureType(), AsDiscrete(to_onehot=True, n_classes=num_class)])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "# 모델에 입력하기 위한 training, validation 또는 test 데이터셋 생성하기 위한 class선언\r\n",
    "\r\n",
    "class MedNISTDataset(torch.utils.data.Dataset): #상속함 ()안에들어있는거\r\n",
    "    def __init__(self, image_files, labels, transforms):\r\n",
    "        self.image_files = image_files\r\n",
    "        self.labels = labels\r\n",
    "        self.transforms = transforms\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.image_files)\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        return self.transforms(self.image_files[index]), self.labels[index]\r\n",
    "\r\n",
    "# data_loader 정의(DataLoader = 데이터셋과 샘플러를 결합하고 사용 가능한 데이터셋을 제공합니다.)\r\n",
    "# num_workers = 학습 도중 CPU의 작업을 몇 개의 코어를 사용해서 진행할지에 대한 설정 파라미터입니다\r\n",
    "# num_workers = 0로 할 경우 모든 코어를 사용하여 진행\r\n",
    "\r\n",
    "train_ds = MedNISTDataset(train_x, train_y, train_transforms)\r\n",
    "train_loader = torch.utils.data.DataLoader(\r\n",
    "    train_ds, batch_size=10, shuffle=True, num_workers=0)\r\n",
    "    # 코드가 멈추는 경우 배치 사이즈를 줄여서 코드 실행\r\n",
    "\r\n",
    "val_ds = MedNISTDataset(val_x, val_y, val_transforms)\r\n",
    "val_loader = torch.utils.data.DataLoader(\r\n",
    "    val_ds, batch_size=10, num_workers=0)\r\n",
    "\r\n",
    "test_ds = MedNISTDataset(test_x, test_y, val_transforms)\r\n",
    "test_loader = torch.utils.data.DataLoader(\r\n",
    "    test_ds, batch_size=10, num_workers=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "device = torch.device(\"cuda:0\") # 저장한 모델을 gpu에서 불러올 때\r\n",
    "                                # gpu가 없는 경우 cuda:0 대신 cpu를 사용 가능 하나 권장 하지 않음\r\n",
    "\r\n",
    "model = DenseNet121(spatial_dims=2, in_channels=4, #DenseNet은 CNN 아키텍쳐 중 하나\r\n",
    "                    out_channels=num_class).to(device) # out_channels = 출력 채널 수\r\n",
    "                    \r\n",
    "loss_function = torch.nn.CrossEntropyLoss() \r\n",
    "optimizer = torch.optim.Adam(model.parameters(), 18e-5) # learning rate\r\n",
    "max_epochs = 6\r\n",
    "val_interval = 1\r\n",
    "auc_metric = ROCAUCMetric() # AUC: ROC 곡선 아래 영역 / AUC 계산 metric"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습이 제대로 안되는 경우 에폭수를 좀더 줄여야 함"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "best_metric = -1    \r\n",
    "best_metric_epoch = -1\r\n",
    "epoch_loss_values = []\r\n",
    "metric_values = []\r\n",
    "\r\n",
    "for epoch in range(max_epochs): \r\n",
    "    print(\"-\" * 10)         #구분선\r\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\") #몇번째 epoch인지 출력\r\n",
    "    model.train()           # 모델 학습\r\n",
    "    epoch_loss = 0          # epoch_loss(epoch의 average loss) 초기화\r\n",
    "    step = 0                # step 초기화\r\n",
    "    for batch_data in train_loader: # train_loader에서 train데이터에 대한 정보가져옴\r\n",
    "        step += 1\r\n",
    "        inputs, labels = batch_data[0].permute(0,3,1,2).to(device), batch_data[1].to(device)\r\n",
    "        optimizer.zero_grad() #Pytorch에서는 gradients값들을 추후에 backward를 해줄때 계속 더해주기 때문\"에 우리는 항상 backpropagation을 하기전에 gradients를 zero로 만들어주고 시작을 해야합니다\r\n",
    "        outputs = model(inputs)\r\n",
    "        loss = loss_function(outputs, labels) #lossfuction으로 loss 계산\r\n",
    "        loss.backward() #현재 tensor의 gradient를 계산\r\n",
    "        optimizer.step() #\r\n",
    "        epoch_loss += loss.item() #train_loss 값을 epoch_loss에 더해서 저장\r\n",
    "        print(\r\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}, \"# train데이터를 batch size로 나눈 값\r\n",
    "            f\"train_loss: {loss.item():.4f}\") # train loss\r\n",
    "        epoch_len = len(train_ds) // train_loader.batch_size #epoch수 (47164 /300)\r\n",
    "    epoch_loss /= step        # average loss는 전체 loss 값 더해준 것을 step으로 나눈 값\r\n",
    "    epoch_loss_values.append(epoch_loss)  # epoch_loss값 리스트에 원소로 추가\r\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\r\n",
    "\r\n",
    "    if (epoch + 1) % val_interval == 0: # epoch 한 번 돌았으면 나머지 수행\r\n",
    "        model.eval()   # 모델 평가\r\n",
    "        with torch.no_grad():    #gradinet 계산하지 않음\r\n",
    "            y_pred = torch.tensor([], dtype=torch.float32, device=device)\r\n",
    "            y = torch.tensor([], dtype=torch.long, device=device)\r\n",
    "            for val_data in val_loader:\r\n",
    "                val_images, val_labels = (\r\n",
    "                    val_data[0].permute(0,3,1,2).to(device),\r\n",
    "                    val_data[1].to(device),\r\n",
    "                )\r\n",
    "                y_pred = torch.cat([y_pred, model(val_images)], dim=0)\r\n",
    "                y = torch.cat([y, val_labels], dim=0)\r\n",
    "            y_onehot = [y_trans(i) for i in decollate_batch(y)]\r\n",
    "            y_pred_act = [y_pred_trans(i) for i in decollate_batch(y_pred)]\r\n",
    "            auc_metric(y_pred_act, y_onehot)\r\n",
    "            result = auc_metric.aggregate() #auc metric을 통해 auc 계산하고 result에 저장\r\n",
    "            auc_metric.reset() #버퍼를 리셋한다\r\n",
    "            del y_pred_act, y_onehot #del은 파이썬 어레이의 요소를 삭제\r\n",
    "            metric_values.append(result) #auc값 list에 추가\r\n",
    "            acc_value = torch.eq(y_pred.argmax(dim=1), y)\r\n",
    "            acc_metric = acc_value.sum().item() / len(acc_value)\r\n",
    "            if result > best_metric:  #best_metric보다 auc크다면 result로 best_metric값 업데이트\r\n",
    "                best_metric = result  #\r\n",
    "                best_metric_epoch = epoch + 1 \r\n",
    "                torch.save(model.state_dict(), os.path.join(  #root_dir에 모델의 weight를 저장\r\n",
    "                    dir, \"best_metric_model2.pth\"))\r\n",
    "                print(\"saved new best metric model\")\r\n",
    "            print(\r\n",
    "                f\"current epoch: {epoch + 1} current AUC: {result:.4f}\"\r\n",
    "                f\" current accuracy: {acc_metric:.4f}\"\r\n",
    "                f\" best AUC: {best_metric:.4f}\"\r\n",
    "                f\" at epoch: {best_metric_epoch}\"\r\n",
    "            )\r\n",
    "\r\n",
    "print(\r\n",
    "    f\"train completed, best_metric: {best_metric:.4f} \"\r\n",
    "    f\"at epoch: {best_metric_epoch}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------\n",
      "epoch 1/6\n",
      "1/2595, train_loss: 0.6836\n",
      "2/2595, train_loss: 0.8108\n",
      "3/2595, train_loss: 0.5463\n",
      "4/2595, train_loss: 0.4855\n",
      "5/2595, train_loss: 0.6860\n",
      "6/2595, train_loss: 0.6312\n",
      "7/2595, train_loss: 0.5491\n",
      "8/2595, train_loss: 0.6293\n",
      "9/2595, train_loss: 0.5219\n",
      "10/2595, train_loss: 0.5537\n",
      "11/2595, train_loss: 0.7100\n",
      "12/2595, train_loss: 0.6166\n",
      "13/2595, train_loss: 0.8749\n",
      "14/2595, train_loss: 0.4511\n",
      "15/2595, train_loss: 0.2629\n",
      "16/2595, train_loss: 0.6473\n",
      "17/2595, train_loss: 0.6939\n",
      "18/2595, train_loss: 0.8364\n",
      "19/2595, train_loss: 0.5316\n",
      "20/2595, train_loss: 0.3990\n",
      "21/2595, train_loss: 0.3144\n",
      "22/2595, train_loss: 0.3079\n",
      "23/2595, train_loss: 0.4176\n",
      "24/2595, train_loss: 0.6084\n",
      "25/2595, train_loss: 0.8153\n",
      "26/2595, train_loss: 0.2404\n",
      "27/2595, train_loss: 0.5262\n",
      "28/2595, train_loss: 0.2907\n",
      "29/2595, train_loss: 0.4980\n",
      "30/2595, train_loss: 0.4074\n",
      "31/2595, train_loss: 0.5556\n",
      "32/2595, train_loss: 0.5151\n",
      "33/2595, train_loss: 0.8170\n",
      "34/2595, train_loss: 0.6786\n",
      "35/2595, train_loss: 0.5902\n",
      "36/2595, train_loss: 0.5251\n",
      "37/2595, train_loss: 0.4703\n",
      "38/2595, train_loss: 1.0216\n",
      "39/2595, train_loss: 0.4481\n",
      "40/2595, train_loss: 0.7940\n",
      "41/2595, train_loss: 0.5914\n",
      "42/2595, train_loss: 0.2581\n",
      "43/2595, train_loss: 0.6220\n",
      "44/2595, train_loss: 0.5012\n",
      "45/2595, train_loss: 0.5010\n",
      "46/2595, train_loss: 0.3116\n",
      "47/2595, train_loss: 0.4162\n",
      "48/2595, train_loss: 0.2391\n",
      "49/2595, train_loss: 0.3912\n",
      "50/2595, train_loss: 0.3572\n",
      "51/2595, train_loss: 0.3464\n",
      "52/2595, train_loss: 0.6257\n",
      "53/2595, train_loss: 0.4035\n",
      "54/2595, train_loss: 0.3490\n",
      "55/2595, train_loss: 0.1896\n",
      "56/2595, train_loss: 0.2128\n",
      "57/2595, train_loss: 0.5445\n",
      "58/2595, train_loss: 0.5776\n",
      "59/2595, train_loss: 0.4434\n",
      "60/2595, train_loss: 0.6639\n",
      "61/2595, train_loss: 0.3342\n",
      "62/2595, train_loss: 0.3198\n",
      "63/2595, train_loss: 0.7078\n",
      "64/2595, train_loss: 0.2929\n",
      "65/2595, train_loss: 0.4574\n",
      "66/2595, train_loss: 0.8780\n",
      "67/2595, train_loss: 0.1854\n",
      "68/2595, train_loss: 0.4014\n",
      "69/2595, train_loss: 0.2348\n",
      "70/2595, train_loss: 0.6995\n",
      "71/2595, train_loss: 0.7656\n",
      "72/2595, train_loss: 0.3174\n",
      "73/2595, train_loss: 0.4773\n",
      "74/2595, train_loss: 0.2426\n",
      "75/2595, train_loss: 0.2757\n",
      "76/2595, train_loss: 0.5281\n",
      "77/2595, train_loss: 0.2007\n",
      "78/2595, train_loss: 0.3196\n",
      "79/2595, train_loss: 0.3670\n",
      "80/2595, train_loss: 0.4582\n",
      "81/2595, train_loss: 0.1411\n",
      "82/2595, train_loss: 0.4431\n",
      "83/2595, train_loss: 0.2709\n",
      "84/2595, train_loss: 0.3503\n",
      "85/2595, train_loss: 0.2712\n",
      "86/2595, train_loss: 0.1575\n",
      "87/2595, train_loss: 0.2470\n",
      "88/2595, train_loss: 0.8898\n",
      "89/2595, train_loss: 0.5536\n",
      "90/2595, train_loss: 0.6886\n",
      "91/2595, train_loss: 0.5343\n",
      "92/2595, train_loss: 0.4012\n",
      "93/2595, train_loss: 0.3168\n",
      "94/2595, train_loss: 0.3247\n",
      "95/2595, train_loss: 0.3635\n",
      "96/2595, train_loss: 0.4604\n",
      "97/2595, train_loss: 0.4404\n",
      "98/2595, train_loss: 0.1517\n",
      "99/2595, train_loss: 0.1638\n",
      "100/2595, train_loss: 0.4988\n",
      "101/2595, train_loss: 0.2893\n",
      "102/2595, train_loss: 0.2859\n",
      "103/2595, train_loss: 0.2855\n",
      "104/2595, train_loss: 0.5790\n",
      "105/2595, train_loss: 0.7317\n",
      "106/2595, train_loss: 0.4314\n",
      "107/2595, train_loss: 0.6167\n",
      "108/2595, train_loss: 1.0237\n",
      "109/2595, train_loss: 0.4012\n",
      "110/2595, train_loss: 0.6671\n",
      "111/2595, train_loss: 0.1698\n",
      "112/2595, train_loss: 0.5652\n",
      "113/2595, train_loss: 0.3700\n",
      "114/2595, train_loss: 0.6692\n",
      "115/2595, train_loss: 1.0702\n",
      "116/2595, train_loss: 0.9622\n",
      "117/2595, train_loss: 0.5255\n",
      "118/2595, train_loss: 1.0766\n",
      "119/2595, train_loss: 0.4495\n",
      "120/2595, train_loss: 0.4905\n",
      "121/2595, train_loss: 0.4852\n",
      "122/2595, train_loss: 0.4221\n",
      "123/2595, train_loss: 0.3533\n",
      "124/2595, train_loss: 0.4071\n",
      "125/2595, train_loss: 0.3630\n",
      "126/2595, train_loss: 0.3461\n",
      "127/2595, train_loss: 0.5341\n",
      "128/2595, train_loss: 0.1845\n",
      "129/2595, train_loss: 0.3413\n",
      "130/2595, train_loss: 0.3371\n",
      "131/2595, train_loss: 0.4449\n",
      "132/2595, train_loss: 0.2948\n",
      "133/2595, train_loss: 0.3826\n",
      "134/2595, train_loss: 0.8345\n",
      "135/2595, train_loss: 0.4160\n",
      "136/2595, train_loss: 0.5151\n",
      "137/2595, train_loss: 0.9111\n",
      "138/2595, train_loss: 0.9870\n",
      "139/2595, train_loss: 0.1872\n",
      "140/2595, train_loss: 0.3179\n",
      "141/2595, train_loss: 0.5640\n",
      "142/2595, train_loss: 0.4097\n",
      "143/2595, train_loss: 0.4260\n",
      "144/2595, train_loss: 0.7280\n",
      "145/2595, train_loss: 0.2109\n",
      "146/2595, train_loss: 0.2340\n",
      "147/2595, train_loss: 0.2624\n",
      "148/2595, train_loss: 0.1977\n",
      "149/2595, train_loss: 0.3856\n",
      "150/2595, train_loss: 0.8910\n",
      "151/2595, train_loss: 0.1254\n",
      "152/2595, train_loss: 0.2335\n",
      "153/2595, train_loss: 0.7748\n",
      "154/2595, train_loss: 0.4600\n",
      "155/2595, train_loss: 0.4971\n",
      "156/2595, train_loss: 0.2112\n",
      "157/2595, train_loss: 0.3510\n",
      "158/2595, train_loss: 0.4459\n",
      "159/2595, train_loss: 0.1294\n",
      "160/2595, train_loss: 0.4643\n",
      "161/2595, train_loss: 0.3152\n",
      "162/2595, train_loss: 1.1960\n",
      "163/2595, train_loss: 0.1428\n",
      "164/2595, train_loss: 0.1224\n",
      "165/2595, train_loss: 0.2767\n",
      "166/2595, train_loss: 0.2034\n",
      "167/2595, train_loss: 0.4869\n",
      "168/2595, train_loss: 0.4572\n",
      "169/2595, train_loss: 0.1788\n",
      "170/2595, train_loss: 0.4997\n",
      "171/2595, train_loss: 0.8759\n",
      "172/2595, train_loss: 0.1714\n",
      "173/2595, train_loss: 0.3071\n",
      "174/2595, train_loss: 0.1294\n",
      "175/2595, train_loss: 0.2705\n",
      "176/2595, train_loss: 0.8581\n",
      "177/2595, train_loss: 0.6525\n",
      "178/2595, train_loss: 0.4525\n",
      "179/2595, train_loss: 0.6408\n",
      "180/2595, train_loss: 0.3513\n",
      "181/2595, train_loss: 0.5064\n",
      "182/2595, train_loss: 0.1772\n",
      "183/2595, train_loss: 0.3021\n",
      "184/2595, train_loss: 0.6177\n",
      "185/2595, train_loss: 0.3186\n",
      "186/2595, train_loss: 0.3850\n",
      "187/2595, train_loss: 0.4127\n",
      "188/2595, train_loss: 0.5389\n",
      "189/2595, train_loss: 0.2452\n",
      "190/2595, train_loss: 0.4292\n",
      "191/2595, train_loss: 0.6949\n",
      "192/2595, train_loss: 0.6278\n",
      "193/2595, train_loss: 0.1972\n",
      "194/2595, train_loss: 1.3069\n",
      "195/2595, train_loss: 0.6589\n",
      "196/2595, train_loss: 0.7247\n",
      "197/2595, train_loss: 0.2361\n",
      "198/2595, train_loss: 0.4850\n",
      "199/2595, train_loss: 0.2997\n",
      "200/2595, train_loss: 0.2537\n",
      "201/2595, train_loss: 0.5538\n",
      "202/2595, train_loss: 0.4077\n",
      "203/2595, train_loss: 0.2590\n",
      "204/2595, train_loss: 0.3725\n",
      "205/2595, train_loss: 0.5596\n",
      "206/2595, train_loss: 0.2166\n",
      "207/2595, train_loss: 0.4155\n",
      "208/2595, train_loss: 0.3859\n",
      "209/2595, train_loss: 0.5120\n",
      "210/2595, train_loss: 0.4166\n",
      "211/2595, train_loss: 0.4205\n",
      "212/2595, train_loss: 0.2079\n",
      "213/2595, train_loss: 0.7721\n",
      "214/2595, train_loss: 0.5900\n",
      "215/2595, train_loss: 0.4167\n",
      "216/2595, train_loss: 0.4768\n",
      "217/2595, train_loss: 0.2688\n",
      "218/2595, train_loss: 0.1976\n",
      "219/2595, train_loss: 0.1759\n",
      "220/2595, train_loss: 0.3067\n",
      "221/2595, train_loss: 0.3729\n",
      "222/2595, train_loss: 0.7068\n",
      "223/2595, train_loss: 0.5326\n",
      "224/2595, train_loss: 0.5331\n",
      "225/2595, train_loss: 0.3311\n",
      "226/2595, train_loss: 0.2331\n",
      "227/2595, train_loss: 0.2870\n",
      "228/2595, train_loss: 0.2309\n",
      "229/2595, train_loss: 0.6131\n",
      "230/2595, train_loss: 0.3274\n",
      "231/2595, train_loss: 0.5979\n",
      "232/2595, train_loss: 0.2315\n",
      "233/2595, train_loss: 0.4627\n",
      "234/2595, train_loss: 0.3359\n",
      "235/2595, train_loss: 0.5240\n",
      "236/2595, train_loss: 0.2848\n",
      "237/2595, train_loss: 0.6652\n",
      "238/2595, train_loss: 0.5107\n",
      "239/2595, train_loss: 0.1461\n",
      "240/2595, train_loss: 0.2023\n",
      "241/2595, train_loss: 0.1539\n",
      "242/2595, train_loss: 0.7647\n",
      "243/2595, train_loss: 0.2381\n",
      "244/2595, train_loss: 0.1851\n",
      "245/2595, train_loss: 0.1529\n",
      "246/2595, train_loss: 0.1960\n",
      "247/2595, train_loss: 0.8795\n",
      "248/2595, train_loss: 0.5500\n",
      "249/2595, train_loss: 0.3885\n",
      "250/2595, train_loss: 1.2431\n",
      "251/2595, train_loss: 0.1962\n",
      "252/2595, train_loss: 0.2782\n",
      "253/2595, train_loss: 0.9804\n",
      "254/2595, train_loss: 1.0507\n",
      "255/2595, train_loss: 0.2236\n",
      "256/2595, train_loss: 0.1911\n",
      "257/2595, train_loss: 0.3296\n",
      "258/2595, train_loss: 0.8399\n",
      "259/2595, train_loss: 0.7269\n",
      "260/2595, train_loss: 0.3217\n",
      "261/2595, train_loss: 0.5575\n",
      "262/2595, train_loss: 1.0787\n",
      "263/2595, train_loss: 0.5273\n",
      "264/2595, train_loss: 0.2801\n",
      "265/2595, train_loss: 0.5935\n",
      "266/2595, train_loss: 0.1891\n",
      "267/2595, train_loss: 0.4769\n",
      "268/2595, train_loss: 0.2787\n",
      "269/2595, train_loss: 0.5197\n",
      "270/2595, train_loss: 0.4158\n",
      "271/2595, train_loss: 0.2800\n",
      "272/2595, train_loss: 0.3798\n",
      "273/2595, train_loss: 0.4135\n",
      "274/2595, train_loss: 0.1627\n",
      "275/2595, train_loss: 0.3384\n",
      "276/2595, train_loss: 0.4680\n",
      "277/2595, train_loss: 0.3171\n",
      "278/2595, train_loss: 0.1190\n",
      "279/2595, train_loss: 0.3059\n",
      "280/2595, train_loss: 0.4109\n",
      "281/2595, train_loss: 0.1625\n",
      "282/2595, train_loss: 0.5291\n",
      "283/2595, train_loss: 0.4267\n",
      "284/2595, train_loss: 0.4581\n",
      "285/2595, train_loss: 0.2634\n",
      "286/2595, train_loss: 0.6596\n",
      "287/2595, train_loss: 0.6355\n",
      "288/2595, train_loss: 0.1590\n",
      "289/2595, train_loss: 0.4822\n",
      "290/2595, train_loss: 0.2304\n",
      "291/2595, train_loss: 0.6559\n",
      "292/2595, train_loss: 0.4697\n",
      "293/2595, train_loss: 0.4077\n",
      "294/2595, train_loss: 0.7842\n",
      "295/2595, train_loss: 0.2003\n",
      "296/2595, train_loss: 0.4590\n",
      "297/2595, train_loss: 0.5380\n",
      "298/2595, train_loss: 0.2110\n",
      "299/2595, train_loss: 0.4109\n",
      "300/2595, train_loss: 0.2052\n",
      "301/2595, train_loss: 0.8799\n",
      "302/2595, train_loss: 0.2842\n",
      "303/2595, train_loss: 0.6869\n",
      "304/2595, train_loss: 0.5680\n",
      "305/2595, train_loss: 0.3358\n",
      "306/2595, train_loss: 0.2885\n",
      "307/2595, train_loss: 0.1611\n",
      "308/2595, train_loss: 0.2879\n",
      "309/2595, train_loss: 0.4943\n",
      "310/2595, train_loss: 0.2367\n",
      "311/2595, train_loss: 0.1735\n",
      "312/2595, train_loss: 0.6206\n",
      "313/2595, train_loss: 0.4995\n",
      "314/2595, train_loss: 0.2951\n",
      "315/2595, train_loss: 0.1128\n",
      "316/2595, train_loss: 0.4504\n",
      "317/2595, train_loss: 0.3633\n",
      "318/2595, train_loss: 0.1945\n",
      "319/2595, train_loss: 0.5991\n",
      "320/2595, train_loss: 0.5090\n",
      "321/2595, train_loss: 0.1407\n",
      "322/2595, train_loss: 0.3227\n",
      "323/2595, train_loss: 0.3698\n",
      "324/2595, train_loss: 0.2273\n",
      "325/2595, train_loss: 0.2453\n",
      "326/2595, train_loss: 0.2948\n",
      "327/2595, train_loss: 0.2332\n",
      "328/2595, train_loss: 0.3966\n",
      "329/2595, train_loss: 0.2249\n",
      "330/2595, train_loss: 0.1390\n",
      "331/2595, train_loss: 0.5030\n",
      "332/2595, train_loss: 0.2573\n",
      "333/2595, train_loss: 0.4736\n",
      "334/2595, train_loss: 0.8007\n",
      "335/2595, train_loss: 0.2602\n",
      "336/2595, train_loss: 0.4258\n",
      "337/2595, train_loss: 0.1090\n",
      "338/2595, train_loss: 0.6349\n",
      "339/2595, train_loss: 0.4510\n",
      "340/2595, train_loss: 0.4041\n",
      "341/2595, train_loss: 0.3147\n",
      "342/2595, train_loss: 0.3324\n",
      "343/2595, train_loss: 0.0953\n",
      "344/2595, train_loss: 0.6308\n",
      "345/2595, train_loss: 0.4477\n",
      "346/2595, train_loss: 0.2749\n",
      "347/2595, train_loss: 0.1520\n",
      "348/2595, train_loss: 0.5522\n",
      "349/2595, train_loss: 0.3304\n",
      "350/2595, train_loss: 0.5596\n",
      "351/2595, train_loss: 0.8640\n",
      "352/2595, train_loss: 0.3964\n",
      "353/2595, train_loss: 0.2582\n",
      "354/2595, train_loss: 0.5096\n",
      "355/2595, train_loss: 0.2228\n",
      "356/2595, train_loss: 0.3656\n",
      "357/2595, train_loss: 0.1592\n",
      "358/2595, train_loss: 0.3875\n",
      "359/2595, train_loss: 0.1078\n",
      "360/2595, train_loss: 0.2392\n",
      "361/2595, train_loss: 0.8705\n",
      "362/2595, train_loss: 0.8107\n",
      "363/2595, train_loss: 0.3198\n",
      "364/2595, train_loss: 0.8250\n",
      "365/2595, train_loss: 0.5572\n",
      "366/2595, train_loss: 0.4047\n",
      "367/2595, train_loss: 0.3337\n",
      "368/2595, train_loss: 0.5913\n",
      "369/2595, train_loss: 0.5579\n",
      "370/2595, train_loss: 0.4628\n",
      "371/2595, train_loss: 0.4334\n",
      "372/2595, train_loss: 0.2251\n",
      "373/2595, train_loss: 0.2841\n",
      "374/2595, train_loss: 0.6501\n",
      "375/2595, train_loss: 0.4807\n",
      "376/2595, train_loss: 0.4648\n",
      "377/2595, train_loss: 0.3017\n",
      "378/2595, train_loss: 0.2311\n",
      "379/2595, train_loss: 0.2521\n",
      "380/2595, train_loss: 0.2330\n",
      "381/2595, train_loss: 0.2388\n",
      "382/2595, train_loss: 0.3908\n",
      "383/2595, train_loss: 0.5199\n",
      "384/2595, train_loss: 0.2623\n",
      "385/2595, train_loss: 0.5503\n",
      "386/2595, train_loss: 0.4123\n",
      "387/2595, train_loss: 0.3988\n",
      "388/2595, train_loss: 0.1735\n",
      "389/2595, train_loss: 0.2296\n",
      "390/2595, train_loss: 0.2337\n",
      "391/2595, train_loss: 0.4721\n",
      "392/2595, train_loss: 0.4943\n",
      "393/2595, train_loss: 0.4454\n",
      "394/2595, train_loss: 0.1086\n",
      "395/2595, train_loss: 0.3216\n",
      "396/2595, train_loss: 0.0829\n",
      "397/2595, train_loss: 0.4465\n",
      "398/2595, train_loss: 0.5964\n",
      "399/2595, train_loss: 0.7542\n",
      "400/2595, train_loss: 0.2591\n",
      "401/2595, train_loss: 0.2195\n",
      "402/2595, train_loss: 0.3562\n",
      "403/2595, train_loss: 0.6651\n",
      "404/2595, train_loss: 0.0841\n",
      "405/2595, train_loss: 0.2089\n",
      "406/2595, train_loss: 0.1006\n",
      "407/2595, train_loss: 0.2559\n",
      "408/2595, train_loss: 0.4284\n",
      "409/2595, train_loss: 0.7433\n",
      "410/2595, train_loss: 0.3451\n",
      "411/2595, train_loss: 0.4043\n",
      "412/2595, train_loss: 0.3047\n",
      "413/2595, train_loss: 0.7890\n",
      "414/2595, train_loss: 0.3413\n",
      "415/2595, train_loss: 0.5980\n",
      "416/2595, train_loss: 0.2172\n",
      "417/2595, train_loss: 0.3032\n",
      "418/2595, train_loss: 0.4270\n",
      "419/2595, train_loss: 0.2765\n",
      "420/2595, train_loss: 0.6252\n",
      "421/2595, train_loss: 0.3612\n",
      "422/2595, train_loss: 0.1835\n",
      "423/2595, train_loss: 0.3988\n",
      "424/2595, train_loss: 1.1683\n",
      "425/2595, train_loss: 0.4406\n",
      "426/2595, train_loss: 0.2586\n",
      "427/2595, train_loss: 0.4231\n",
      "428/2595, train_loss: 0.6535\n",
      "429/2595, train_loss: 0.4672\n",
      "430/2595, train_loss: 0.1779\n",
      "431/2595, train_loss: 0.7897\n",
      "432/2595, train_loss: 0.1191\n",
      "433/2595, train_loss: 0.3159\n",
      "434/2595, train_loss: 0.4773\n",
      "435/2595, train_loss: 0.6852\n",
      "436/2595, train_loss: 0.5358\n",
      "437/2595, train_loss: 0.7658\n",
      "438/2595, train_loss: 0.1802\n",
      "439/2595, train_loss: 0.1931\n",
      "440/2595, train_loss: 0.4331\n",
      "441/2595, train_loss: 0.1548\n",
      "442/2595, train_loss: 0.6989\n",
      "443/2595, train_loss: 0.3724\n",
      "444/2595, train_loss: 0.2088\n",
      "445/2595, train_loss: 1.2832\n",
      "446/2595, train_loss: 0.7300\n",
      "447/2595, train_loss: 0.8487\n",
      "448/2595, train_loss: 0.5780\n",
      "449/2595, train_loss: 0.5911\n",
      "450/2595, train_loss: 0.6478\n",
      "451/2595, train_loss: 0.4759\n",
      "452/2595, train_loss: 0.3218\n",
      "453/2595, train_loss: 0.4122\n",
      "454/2595, train_loss: 0.6145\n",
      "455/2595, train_loss: 0.4754\n",
      "456/2595, train_loss: 0.2727\n",
      "457/2595, train_loss: 0.6797\n",
      "458/2595, train_loss: 0.3359\n",
      "459/2595, train_loss: 0.2936\n",
      "460/2595, train_loss: 0.3092\n",
      "461/2595, train_loss: 0.3893\n",
      "462/2595, train_loss: 0.2996\n",
      "463/2595, train_loss: 0.2622\n",
      "464/2595, train_loss: 0.2468\n",
      "465/2595, train_loss: 0.5105\n",
      "466/2595, train_loss: 0.4718\n",
      "467/2595, train_loss: 0.3024\n",
      "468/2595, train_loss: 0.9966\n",
      "469/2595, train_loss: 0.1125\n",
      "470/2595, train_loss: 0.4504\n",
      "471/2595, train_loss: 0.6119\n",
      "472/2595, train_loss: 0.3254\n",
      "473/2595, train_loss: 0.3191\n",
      "474/2595, train_loss: 0.3112\n",
      "475/2595, train_loss: 0.8715\n",
      "476/2595, train_loss: 0.1357\n",
      "477/2595, train_loss: 0.2580\n",
      "478/2595, train_loss: 0.4363\n",
      "479/2595, train_loss: 0.2039\n",
      "480/2595, train_loss: 0.4126\n",
      "481/2595, train_loss: 0.2565\n",
      "482/2595, train_loss: 0.5670\n",
      "483/2595, train_loss: 0.3346\n",
      "484/2595, train_loss: 0.1634\n",
      "485/2595, train_loss: 0.4658\n",
      "486/2595, train_loss: 0.2737\n",
      "487/2595, train_loss: 0.5950\n",
      "488/2595, train_loss: 0.1723\n",
      "489/2595, train_loss: 0.8289\n",
      "490/2595, train_loss: 0.7040\n",
      "491/2595, train_loss: 0.4130\n",
      "492/2595, train_loss: 0.2927\n",
      "493/2595, train_loss: 0.1495\n",
      "494/2595, train_loss: 0.6657\n",
      "495/2595, train_loss: 0.3714\n",
      "496/2595, train_loss: 0.2452\n",
      "497/2595, train_loss: 0.2726\n",
      "498/2595, train_loss: 0.4474\n",
      "499/2595, train_loss: 0.2678\n",
      "500/2595, train_loss: 0.2986\n",
      "501/2595, train_loss: 0.3315\n",
      "502/2595, train_loss: 0.1897\n",
      "503/2595, train_loss: 0.3406\n",
      "504/2595, train_loss: 0.9992\n",
      "505/2595, train_loss: 0.5140\n",
      "506/2595, train_loss: 0.3955\n",
      "507/2595, train_loss: 0.3436\n",
      "508/2595, train_loss: 0.4555\n",
      "509/2595, train_loss: 0.4737\n",
      "510/2595, train_loss: 0.3364\n",
      "511/2595, train_loss: 0.7380\n",
      "512/2595, train_loss: 0.3756\n",
      "513/2595, train_loss: 0.5345\n",
      "514/2595, train_loss: 0.3519\n",
      "515/2595, train_loss: 0.4829\n",
      "516/2595, train_loss: 0.2920\n",
      "517/2595, train_loss: 0.2034\n",
      "518/2595, train_loss: 0.1863\n",
      "519/2595, train_loss: 0.3028\n",
      "520/2595, train_loss: 0.1970\n",
      "521/2595, train_loss: 0.2346\n",
      "522/2595, train_loss: 0.2308\n",
      "523/2595, train_loss: 0.1242\n",
      "524/2595, train_loss: 0.3132\n",
      "525/2595, train_loss: 0.4764\n",
      "526/2595, train_loss: 0.5145\n",
      "527/2595, train_loss: 0.3769\n",
      "528/2595, train_loss: 0.4180\n",
      "529/2595, train_loss: 0.3322\n",
      "530/2595, train_loss: 0.6312\n",
      "531/2595, train_loss: 0.5411\n",
      "532/2595, train_loss: 0.2875\n",
      "533/2595, train_loss: 0.4982\n",
      "534/2595, train_loss: 0.7837\n",
      "535/2595, train_loss: 0.3435\n",
      "536/2595, train_loss: 0.1151\n",
      "537/2595, train_loss: 0.3994\n",
      "538/2595, train_loss: 0.3201\n",
      "539/2595, train_loss: 0.2634\n",
      "540/2595, train_loss: 0.3352\n",
      "541/2595, train_loss: 0.1252\n",
      "542/2595, train_loss: 0.3840\n",
      "543/2595, train_loss: 0.6083\n",
      "544/2595, train_loss: 0.3077\n",
      "545/2595, train_loss: 0.3287\n",
      "546/2595, train_loss: 0.2834\n",
      "547/2595, train_loss: 0.3028\n",
      "548/2595, train_loss: 0.2040\n",
      "549/2595, train_loss: 0.4841\n",
      "550/2595, train_loss: 0.3986\n",
      "551/2595, train_loss: 1.3527\n",
      "552/2595, train_loss: 0.1288\n",
      "553/2595, train_loss: 0.8025\n",
      "554/2595, train_loss: 0.6652\n",
      "555/2595, train_loss: 0.4299\n",
      "556/2595, train_loss: 0.3281\n",
      "557/2595, train_loss: 0.3790\n",
      "558/2595, train_loss: 0.2099\n",
      "559/2595, train_loss: 0.3482\n",
      "560/2595, train_loss: 0.3875\n",
      "561/2595, train_loss: 0.5025\n",
      "562/2595, train_loss: 0.4428\n",
      "563/2595, train_loss: 0.5151\n",
      "564/2595, train_loss: 0.3110\n",
      "565/2595, train_loss: 0.5260\n",
      "566/2595, train_loss: 0.2458\n",
      "567/2595, train_loss: 0.2673\n",
      "568/2595, train_loss: 0.9170\n",
      "569/2595, train_loss: 0.5832\n",
      "570/2595, train_loss: 0.5520\n",
      "571/2595, train_loss: 0.3578\n",
      "572/2595, train_loss: 0.5160\n",
      "573/2595, train_loss: 0.3540\n",
      "574/2595, train_loss: 0.6075\n",
      "575/2595, train_loss: 0.3711\n",
      "576/2595, train_loss: 0.2522\n",
      "577/2595, train_loss: 0.2471\n",
      "578/2595, train_loss: 0.3204\n",
      "579/2595, train_loss: 0.4241\n",
      "580/2595, train_loss: 0.3673\n",
      "581/2595, train_loss: 0.2286\n",
      "582/2595, train_loss: 0.4552\n",
      "583/2595, train_loss: 0.8294\n",
      "584/2595, train_loss: 0.1241\n",
      "585/2595, train_loss: 0.3915\n",
      "586/2595, train_loss: 0.5415\n",
      "587/2595, train_loss: 0.1005\n",
      "588/2595, train_loss: 0.1387\n",
      "589/2595, train_loss: 0.2788\n",
      "590/2595, train_loss: 0.4536\n",
      "591/2595, train_loss: 0.1831\n",
      "592/2595, train_loss: 0.1518\n",
      "593/2595, train_loss: 0.6906\n",
      "594/2595, train_loss: 0.4650\n",
      "595/2595, train_loss: 0.3680\n",
      "596/2595, train_loss: 0.2376\n",
      "597/2595, train_loss: 0.2460\n",
      "598/2595, train_loss: 0.2062\n",
      "599/2595, train_loss: 0.1569\n",
      "600/2595, train_loss: 0.3214\n",
      "601/2595, train_loss: 0.1245\n",
      "602/2595, train_loss: 0.1284\n",
      "603/2595, train_loss: 1.3090\n",
      "604/2595, train_loss: 0.7516\n",
      "605/2595, train_loss: 0.2195\n",
      "606/2595, train_loss: 0.5339\n",
      "607/2595, train_loss: 0.1669\n",
      "608/2595, train_loss: 0.3390\n",
      "609/2595, train_loss: 0.3459\n",
      "610/2595, train_loss: 0.3478\n",
      "611/2595, train_loss: 0.6505\n",
      "612/2595, train_loss: 0.2950\n",
      "613/2595, train_loss: 0.4529\n",
      "614/2595, train_loss: 0.3768\n",
      "615/2595, train_loss: 0.4115\n",
      "616/2595, train_loss: 0.7151\n",
      "617/2595, train_loss: 0.4415\n",
      "618/2595, train_loss: 0.2062\n",
      "619/2595, train_loss: 0.2044\n",
      "620/2595, train_loss: 0.2794\n",
      "621/2595, train_loss: 0.2753\n",
      "622/2595, train_loss: 0.3953\n",
      "623/2595, train_loss: 0.3344\n",
      "624/2595, train_loss: 0.6460\n",
      "625/2595, train_loss: 0.3304\n",
      "626/2595, train_loss: 0.4871\n",
      "627/2595, train_loss: 0.3058\n",
      "628/2595, train_loss: 0.3634\n",
      "629/2595, train_loss: 0.2296\n",
      "630/2595, train_loss: 0.2291\n",
      "631/2595, train_loss: 0.4830\n",
      "632/2595, train_loss: 0.1785\n",
      "633/2595, train_loss: 0.5638\n",
      "634/2595, train_loss: 0.2364\n",
      "635/2595, train_loss: 0.4476\n",
      "636/2595, train_loss: 0.4061\n",
      "637/2595, train_loss: 0.4327\n",
      "638/2595, train_loss: 0.1052\n",
      "639/2595, train_loss: 0.5282\n",
      "640/2595, train_loss: 0.5807\n",
      "641/2595, train_loss: 0.6074\n",
      "642/2595, train_loss: 0.4245\n",
      "643/2595, train_loss: 0.5452\n",
      "644/2595, train_loss: 0.2610\n",
      "645/2595, train_loss: 0.6774\n",
      "646/2595, train_loss: 0.2846\n",
      "647/2595, train_loss: 0.2145\n",
      "648/2595, train_loss: 0.3121\n",
      "649/2595, train_loss: 0.0985\n",
      "650/2595, train_loss: 0.1100\n",
      "651/2595, train_loss: 0.8055\n",
      "652/2595, train_loss: 0.3250\n",
      "653/2595, train_loss: 0.2554\n",
      "654/2595, train_loss: 0.2869\n",
      "655/2595, train_loss: 0.2200\n",
      "656/2595, train_loss: 0.1217\n",
      "657/2595, train_loss: 0.1061\n",
      "658/2595, train_loss: 0.1477\n",
      "659/2595, train_loss: 0.2438\n",
      "660/2595, train_loss: 0.5440\n",
      "661/2595, train_loss: 0.3288\n",
      "662/2595, train_loss: 0.2156\n",
      "663/2595, train_loss: 0.5516\n",
      "664/2595, train_loss: 0.3633\n",
      "665/2595, train_loss: 0.3037\n",
      "666/2595, train_loss: 0.3436\n",
      "667/2595, train_loss: 0.7203\n",
      "668/2595, train_loss: 0.3858\n",
      "669/2595, train_loss: 0.6171\n",
      "670/2595, train_loss: 0.4002\n",
      "671/2595, train_loss: 0.2884\n",
      "672/2595, train_loss: 0.2079\n",
      "673/2595, train_loss: 0.2886\n",
      "674/2595, train_loss: 0.5942\n",
      "675/2595, train_loss: 0.1640\n",
      "676/2595, train_loss: 0.1617\n",
      "677/2595, train_loss: 0.4157\n",
      "678/2595, train_loss: 0.4676\n",
      "679/2595, train_loss: 0.5819\n",
      "680/2595, train_loss: 0.6978\n",
      "681/2595, train_loss: 0.2285\n",
      "682/2595, train_loss: 0.2472\n",
      "683/2595, train_loss: 0.1730\n",
      "684/2595, train_loss: 0.1297\n",
      "685/2595, train_loss: 0.6390\n",
      "686/2595, train_loss: 0.4652\n",
      "687/2595, train_loss: 0.5201\n",
      "688/2595, train_loss: 0.7048\n",
      "689/2595, train_loss: 0.3320\n",
      "690/2595, train_loss: 0.4203\n",
      "691/2595, train_loss: 0.3447\n",
      "692/2595, train_loss: 0.7013\n",
      "693/2595, train_loss: 0.4967\n",
      "694/2595, train_loss: 0.2002\n",
      "695/2595, train_loss: 0.2512\n",
      "696/2595, train_loss: 0.1606\n",
      "697/2595, train_loss: 0.6077\n",
      "698/2595, train_loss: 0.4022\n",
      "699/2595, train_loss: 0.3233\n",
      "700/2595, train_loss: 0.6547\n",
      "701/2595, train_loss: 0.3608\n",
      "702/2595, train_loss: 0.5692\n",
      "703/2595, train_loss: 0.3416\n",
      "704/2595, train_loss: 0.4483\n",
      "705/2595, train_loss: 0.2790\n",
      "706/2595, train_loss: 0.5231\n",
      "707/2595, train_loss: 0.3709\n",
      "708/2595, train_loss: 0.4038\n",
      "709/2595, train_loss: 0.2558\n",
      "710/2595, train_loss: 0.6055\n",
      "711/2595, train_loss: 0.2484\n",
      "712/2595, train_loss: 0.3343\n",
      "713/2595, train_loss: 0.2675\n",
      "714/2595, train_loss: 0.3289\n",
      "715/2595, train_loss: 0.5298\n",
      "716/2595, train_loss: 0.4315\n",
      "717/2595, train_loss: 0.3922\n",
      "718/2595, train_loss: 0.3288\n",
      "719/2595, train_loss: 0.3058\n",
      "720/2595, train_loss: 0.4600\n",
      "721/2595, train_loss: 0.3115\n",
      "722/2595, train_loss: 0.4649\n",
      "723/2595, train_loss: 0.4859\n",
      "724/2595, train_loss: 0.4616\n",
      "725/2595, train_loss: 0.2899\n",
      "726/2595, train_loss: 0.1721\n",
      "727/2595, train_loss: 0.2378\n",
      "728/2595, train_loss: 0.1851\n",
      "729/2595, train_loss: 0.2370\n",
      "730/2595, train_loss: 0.2305\n",
      "731/2595, train_loss: 0.4578\n",
      "732/2595, train_loss: 0.3953\n",
      "733/2595, train_loss: 0.2443\n",
      "734/2595, train_loss: 0.1962\n",
      "735/2595, train_loss: 0.7800\n",
      "736/2595, train_loss: 0.7244\n",
      "737/2595, train_loss: 0.2872\n",
      "738/2595, train_loss: 0.5339\n",
      "739/2595, train_loss: 0.4339\n",
      "740/2595, train_loss: 0.5755\n",
      "741/2595, train_loss: 0.3887\n",
      "742/2595, train_loss: 0.3014\n",
      "743/2595, train_loss: 0.7131\n",
      "744/2595, train_loss: 0.6589\n",
      "745/2595, train_loss: 0.4549\n",
      "746/2595, train_loss: 0.9267\n",
      "747/2595, train_loss: 0.2778\n",
      "748/2595, train_loss: 0.2037\n",
      "749/2595, train_loss: 0.3089\n",
      "750/2595, train_loss: 0.1832\n",
      "751/2595, train_loss: 0.1824\n",
      "752/2595, train_loss: 0.4733\n",
      "753/2595, train_loss: 0.3346\n",
      "754/2595, train_loss: 0.1543\n",
      "755/2595, train_loss: 0.1537\n",
      "756/2595, train_loss: 0.6947\n",
      "757/2595, train_loss: 0.4221\n",
      "758/2595, train_loss: 0.2675\n",
      "759/2595, train_loss: 0.7413\n",
      "760/2595, train_loss: 0.5508\n",
      "761/2595, train_loss: 0.3621\n",
      "762/2595, train_loss: 0.3599\n",
      "763/2595, train_loss: 0.7515\n",
      "764/2595, train_loss: 0.2028\n",
      "765/2595, train_loss: 0.7759\n",
      "766/2595, train_loss: 0.4035\n",
      "767/2595, train_loss: 0.8877\n",
      "768/2595, train_loss: 0.6438\n",
      "769/2595, train_loss: 0.3205\n",
      "770/2595, train_loss: 0.4581\n",
      "771/2595, train_loss: 0.4014\n",
      "772/2595, train_loss: 0.3337\n",
      "773/2595, train_loss: 0.3983\n",
      "774/2595, train_loss: 0.4848\n",
      "775/2595, train_loss: 0.2734\n",
      "776/2595, train_loss: 0.1959\n",
      "777/2595, train_loss: 0.2318\n",
      "778/2595, train_loss: 0.6235\n",
      "779/2595, train_loss: 0.3351\n",
      "780/2595, train_loss: 0.7696\n",
      "781/2595, train_loss: 0.3880\n",
      "782/2595, train_loss: 0.6442\n",
      "783/2595, train_loss: 0.3611\n",
      "784/2595, train_loss: 0.2047\n",
      "785/2595, train_loss: 0.1444\n",
      "786/2595, train_loss: 0.4472\n",
      "787/2595, train_loss: 0.2459\n",
      "788/2595, train_loss: 0.2493\n",
      "789/2595, train_loss: 0.1336\n",
      "790/2595, train_loss: 0.4210\n",
      "791/2595, train_loss: 0.2093\n",
      "792/2595, train_loss: 0.7524\n",
      "793/2595, train_loss: 0.2973\n",
      "794/2595, train_loss: 0.7479\n",
      "795/2595, train_loss: 0.3134\n",
      "796/2595, train_loss: 0.2946\n",
      "797/2595, train_loss: 0.5511\n",
      "798/2595, train_loss: 0.4610\n",
      "799/2595, train_loss: 0.4402\n",
      "800/2595, train_loss: 0.3753\n",
      "801/2595, train_loss: 0.2292\n",
      "802/2595, train_loss: 0.1343\n",
      "803/2595, train_loss: 0.0947\n",
      "804/2595, train_loss: 0.3109\n",
      "805/2595, train_loss: 0.3351\n",
      "806/2595, train_loss: 0.3872\n",
      "807/2595, train_loss: 0.8103\n",
      "808/2595, train_loss: 0.5760\n",
      "809/2595, train_loss: 0.4310\n",
      "810/2595, train_loss: 0.3183\n",
      "811/2595, train_loss: 0.0901\n",
      "812/2595, train_loss: 0.5676\n",
      "813/2595, train_loss: 0.4461\n",
      "814/2595, train_loss: 0.5842\n",
      "815/2595, train_loss: 0.1722\n",
      "816/2595, train_loss: 0.4824\n",
      "817/2595, train_loss: 0.3852\n",
      "818/2595, train_loss: 0.0998\n",
      "819/2595, train_loss: 0.1784\n",
      "820/2595, train_loss: 0.6552\n",
      "821/2595, train_loss: 0.3449\n",
      "822/2595, train_loss: 0.1907\n",
      "823/2595, train_loss: 0.4744\n",
      "824/2595, train_loss: 0.2972\n",
      "825/2595, train_loss: 0.3149\n",
      "826/2595, train_loss: 0.1685\n",
      "827/2595, train_loss: 0.2153\n",
      "828/2595, train_loss: 0.3860\n",
      "829/2595, train_loss: 0.1812\n",
      "830/2595, train_loss: 0.6293\n",
      "831/2595, train_loss: 0.1364\n",
      "832/2595, train_loss: 0.5819\n",
      "833/2595, train_loss: 0.2953\n",
      "834/2595, train_loss: 0.3206\n",
      "835/2595, train_loss: 0.3561\n",
      "836/2595, train_loss: 0.8303\n",
      "837/2595, train_loss: 0.3927\n",
      "838/2595, train_loss: 0.2425\n",
      "839/2595, train_loss: 0.2953\n",
      "840/2595, train_loss: 0.5522\n",
      "841/2595, train_loss: 0.3587\n",
      "842/2595, train_loss: 0.2155\n",
      "843/2595, train_loss: 1.0515\n",
      "844/2595, train_loss: 0.5799\n",
      "845/2595, train_loss: 0.1631\n",
      "846/2595, train_loss: 0.3467\n",
      "847/2595, train_loss: 0.2772\n",
      "848/2595, train_loss: 0.4029\n",
      "849/2595, train_loss: 0.3905\n",
      "850/2595, train_loss: 0.5265\n",
      "851/2595, train_loss: 0.4245\n",
      "852/2595, train_loss: 0.3706\n",
      "853/2595, train_loss: 0.3808\n",
      "854/2595, train_loss: 0.3394\n",
      "855/2595, train_loss: 0.2795\n",
      "856/2595, train_loss: 0.4308\n",
      "857/2595, train_loss: 0.3304\n",
      "858/2595, train_loss: 0.1609\n",
      "859/2595, train_loss: 0.6074\n",
      "860/2595, train_loss: 0.2530\n",
      "861/2595, train_loss: 0.3562\n",
      "862/2595, train_loss: 0.1220\n",
      "863/2595, train_loss: 0.4715\n",
      "864/2595, train_loss: 0.1804\n",
      "865/2595, train_loss: 0.3596\n",
      "866/2595, train_loss: 0.2163\n",
      "867/2595, train_loss: 0.6856\n",
      "868/2595, train_loss: 0.1778\n",
      "869/2595, train_loss: 0.5300\n",
      "870/2595, train_loss: 0.3578\n",
      "871/2595, train_loss: 0.2379\n",
      "872/2595, train_loss: 0.4064\n",
      "873/2595, train_loss: 0.1262\n",
      "874/2595, train_loss: 0.3691\n",
      "875/2595, train_loss: 0.1594\n",
      "876/2595, train_loss: 0.5891\n",
      "877/2595, train_loss: 0.6566\n",
      "878/2595, train_loss: 0.3283\n",
      "879/2595, train_loss: 0.6083\n",
      "880/2595, train_loss: 0.2338\n",
      "881/2595, train_loss: 0.3982\n",
      "882/2595, train_loss: 0.1728\n",
      "883/2595, train_loss: 0.1778\n",
      "884/2595, train_loss: 0.4025\n",
      "885/2595, train_loss: 0.2299\n",
      "886/2595, train_loss: 0.1720\n",
      "887/2595, train_loss: 0.4272\n",
      "888/2595, train_loss: 0.8874\n",
      "889/2595, train_loss: 0.3986\n",
      "890/2595, train_loss: 0.3376\n",
      "891/2595, train_loss: 0.4069\n",
      "892/2595, train_loss: 0.4790\n",
      "893/2595, train_loss: 0.5571\n",
      "894/2595, train_loss: 0.4192\n",
      "895/2595, train_loss: 0.3164\n",
      "896/2595, train_loss: 0.3699\n",
      "897/2595, train_loss: 0.2643\n",
      "898/2595, train_loss: 0.4692\n",
      "899/2595, train_loss: 0.3120\n",
      "900/2595, train_loss: 0.7765\n",
      "901/2595, train_loss: 0.2555\n",
      "902/2595, train_loss: 0.5000\n",
      "903/2595, train_loss: 0.2802\n",
      "904/2595, train_loss: 0.5351\n",
      "905/2595, train_loss: 0.1742\n",
      "906/2595, train_loss: 0.5828\n",
      "907/2595, train_loss: 0.5657\n",
      "908/2595, train_loss: 0.2537\n",
      "909/2595, train_loss: 0.5985\n",
      "910/2595, train_loss: 0.2306\n",
      "911/2595, train_loss: 0.6264\n",
      "912/2595, train_loss: 0.5563\n",
      "913/2595, train_loss: 0.2986\n",
      "914/2595, train_loss: 0.7212\n",
      "915/2595, train_loss: 0.5402\n",
      "916/2595, train_loss: 0.2493\n",
      "917/2595, train_loss: 0.4541\n",
      "918/2595, train_loss: 0.7279\n",
      "919/2595, train_loss: 0.4693\n",
      "920/2595, train_loss: 0.5119\n",
      "921/2595, train_loss: 0.3934\n",
      "922/2595, train_loss: 0.1851\n",
      "923/2595, train_loss: 0.3180\n",
      "924/2595, train_loss: 0.3236\n",
      "925/2595, train_loss: 0.2528\n",
      "926/2595, train_loss: 0.3033\n",
      "927/2595, train_loss: 0.2186\n",
      "928/2595, train_loss: 0.2142\n",
      "929/2595, train_loss: 0.2259\n",
      "930/2595, train_loss: 0.4596\n",
      "931/2595, train_loss: 0.3477\n",
      "932/2595, train_loss: 0.6579\n",
      "933/2595, train_loss: 0.3938\n",
      "934/2595, train_loss: 0.4811\n",
      "935/2595, train_loss: 0.4362\n",
      "936/2595, train_loss: 0.1904\n",
      "937/2595, train_loss: 0.6249\n",
      "938/2595, train_loss: 0.3648\n",
      "939/2595, train_loss: 0.1739\n",
      "940/2595, train_loss: 0.4920\n",
      "941/2595, train_loss: 0.0773\n",
      "942/2595, train_loss: 0.4673\n",
      "943/2595, train_loss: 0.3466\n",
      "944/2595, train_loss: 0.1982\n",
      "945/2595, train_loss: 0.3605\n",
      "946/2595, train_loss: 0.3971\n",
      "947/2595, train_loss: 0.5109\n",
      "948/2595, train_loss: 0.2778\n",
      "949/2595, train_loss: 0.3929\n",
      "950/2595, train_loss: 0.1710\n",
      "951/2595, train_loss: 0.1409\n",
      "952/2595, train_loss: 0.3245\n",
      "953/2595, train_loss: 0.2809\n",
      "954/2595, train_loss: 0.3618\n",
      "955/2595, train_loss: 0.3177\n",
      "956/2595, train_loss: 0.1104\n",
      "957/2595, train_loss: 0.3756\n",
      "958/2595, train_loss: 0.3122\n",
      "959/2595, train_loss: 0.1078\n",
      "960/2595, train_loss: 0.6482\n",
      "961/2595, train_loss: 0.1163\n",
      "962/2595, train_loss: 0.2727\n",
      "963/2595, train_loss: 0.0862\n",
      "964/2595, train_loss: 0.5644\n",
      "965/2595, train_loss: 0.4523\n",
      "966/2595, train_loss: 0.0674\n",
      "967/2595, train_loss: 0.6751\n",
      "968/2595, train_loss: 0.1009\n",
      "969/2595, train_loss: 0.0825\n",
      "970/2595, train_loss: 0.2637\n",
      "971/2595, train_loss: 0.4360\n",
      "972/2595, train_loss: 0.2668\n",
      "973/2595, train_loss: 0.4372\n",
      "974/2595, train_loss: 0.6109\n",
      "975/2595, train_loss: 0.6489\n",
      "976/2595, train_loss: 0.4118\n",
      "977/2595, train_loss: 0.5773\n",
      "978/2595, train_loss: 0.5531\n",
      "979/2595, train_loss: 0.5574\n",
      "980/2595, train_loss: 0.1963\n",
      "981/2595, train_loss: 0.0891\n",
      "982/2595, train_loss: 0.8474\n",
      "983/2595, train_loss: 0.1647\n",
      "984/2595, train_loss: 0.1779\n",
      "985/2595, train_loss: 0.1890\n",
      "986/2595, train_loss: 0.5628\n",
      "987/2595, train_loss: 0.1539\n",
      "988/2595, train_loss: 0.2779\n",
      "989/2595, train_loss: 0.3649\n",
      "990/2595, train_loss: 0.1650\n",
      "991/2595, train_loss: 0.4715\n",
      "992/2595, train_loss: 0.5812\n",
      "993/2595, train_loss: 0.1708\n",
      "994/2595, train_loss: 0.1344\n",
      "995/2595, train_loss: 0.2043\n",
      "996/2595, train_loss: 0.3764\n",
      "997/2595, train_loss: 0.3090\n",
      "998/2595, train_loss: 0.4073\n",
      "999/2595, train_loss: 0.2509\n",
      "1000/2595, train_loss: 0.3348\n",
      "1001/2595, train_loss: 0.2140\n",
      "1002/2595, train_loss: 0.4801\n",
      "1003/2595, train_loss: 0.4482\n",
      "1004/2595, train_loss: 0.1674\n",
      "1005/2595, train_loss: 0.2165\n",
      "1006/2595, train_loss: 0.3720\n",
      "1007/2595, train_loss: 0.3629\n",
      "1008/2595, train_loss: 0.2437\n",
      "1009/2595, train_loss: 0.1274\n",
      "1010/2595, train_loss: 0.1985\n",
      "1011/2595, train_loss: 0.2574\n",
      "1012/2595, train_loss: 0.3685\n",
      "1013/2595, train_loss: 0.2260\n",
      "1014/2595, train_loss: 0.5940\n",
      "1015/2595, train_loss: 0.0708\n",
      "1016/2595, train_loss: 0.4221\n",
      "1017/2595, train_loss: 0.3470\n",
      "1018/2595, train_loss: 0.0859\n",
      "1019/2595, train_loss: 0.2148\n",
      "1020/2595, train_loss: 0.1606\n",
      "1021/2595, train_loss: 0.4056\n",
      "1022/2595, train_loss: 0.4113\n",
      "1023/2595, train_loss: 0.2724\n",
      "1024/2595, train_loss: 0.2362\n",
      "1025/2595, train_loss: 0.1870\n",
      "1026/2595, train_loss: 0.7495\n",
      "1027/2595, train_loss: 0.1385\n",
      "1028/2595, train_loss: 0.2987\n",
      "1029/2595, train_loss: 0.2084\n",
      "1030/2595, train_loss: 0.5394\n",
      "1031/2595, train_loss: 1.2367\n",
      "1032/2595, train_loss: 0.3292\n",
      "1033/2595, train_loss: 0.3689\n",
      "1034/2595, train_loss: 0.2210\n",
      "1035/2595, train_loss: 0.3978\n",
      "1036/2595, train_loss: 0.1000\n",
      "1037/2595, train_loss: 0.1232\n",
      "1038/2595, train_loss: 0.1963\n",
      "1039/2595, train_loss: 0.5903\n",
      "1040/2595, train_loss: 0.7569\n",
      "1041/2595, train_loss: 0.2284\n",
      "1042/2595, train_loss: 0.3075\n",
      "1043/2595, train_loss: 0.4646\n",
      "1044/2595, train_loss: 0.0987\n",
      "1045/2595, train_loss: 0.2282\n",
      "1046/2595, train_loss: 0.2417\n",
      "1047/2595, train_loss: 0.2002\n",
      "1048/2595, train_loss: 0.9583\n",
      "1049/2595, train_loss: 0.1324\n",
      "1050/2595, train_loss: 0.2024\n",
      "1051/2595, train_loss: 0.5198\n",
      "1052/2595, train_loss: 0.3030\n",
      "1053/2595, train_loss: 0.3533\n",
      "1054/2595, train_loss: 0.0998\n",
      "1055/2595, train_loss: 0.3415\n",
      "1056/2595, train_loss: 0.1806\n",
      "1057/2595, train_loss: 0.4542\n",
      "1058/2595, train_loss: 0.7599\n",
      "1059/2595, train_loss: 0.2164\n",
      "1060/2595, train_loss: 0.4902\n",
      "1061/2595, train_loss: 0.0981\n",
      "1062/2595, train_loss: 0.0610\n",
      "1063/2595, train_loss: 0.1255\n",
      "1064/2595, train_loss: 0.2757\n",
      "1065/2595, train_loss: 0.1147\n",
      "1066/2595, train_loss: 0.2657\n",
      "1067/2595, train_loss: 0.4098\n",
      "1068/2595, train_loss: 0.2985\n",
      "1069/2595, train_loss: 0.2074\n",
      "1070/2595, train_loss: 0.2043\n",
      "1071/2595, train_loss: 0.3262\n",
      "1072/2595, train_loss: 0.2500\n",
      "1073/2595, train_loss: 0.7089\n",
      "1074/2595, train_loss: 0.1745\n",
      "1075/2595, train_loss: 0.0962\n",
      "1076/2595, train_loss: 0.1864\n",
      "1077/2595, train_loss: 0.4025\n",
      "1078/2595, train_loss: 0.2432\n",
      "1079/2595, train_loss: 0.1734\n",
      "1080/2595, train_loss: 0.8601\n",
      "1081/2595, train_loss: 0.4516\n",
      "1082/2595, train_loss: 0.3743\n",
      "1083/2595, train_loss: 0.7222\n",
      "1084/2595, train_loss: 0.3460\n",
      "1085/2595, train_loss: 0.3565\n",
      "1086/2595, train_loss: 0.5285\n",
      "1087/2595, train_loss: 0.5827\n",
      "1088/2595, train_loss: 0.6075\n",
      "1089/2595, train_loss: 0.5114\n",
      "1090/2595, train_loss: 0.3377\n",
      "1091/2595, train_loss: 0.3152\n",
      "1092/2595, train_loss: 0.5520\n",
      "1093/2595, train_loss: 0.2480\n",
      "1094/2595, train_loss: 0.2184\n",
      "1095/2595, train_loss: 0.5210\n",
      "1096/2595, train_loss: 0.3126\n",
      "1097/2595, train_loss: 0.2315\n",
      "1098/2595, train_loss: 0.3089\n",
      "1099/2595, train_loss: 0.3919\n",
      "1100/2595, train_loss: 0.6210\n",
      "1101/2595, train_loss: 0.2083\n",
      "1102/2595, train_loss: 0.3993\n",
      "1103/2595, train_loss: 0.2333\n",
      "1104/2595, train_loss: 0.4663\n",
      "1105/2595, train_loss: 0.2088\n",
      "1106/2595, train_loss: 0.3305\n",
      "1107/2595, train_loss: 0.1905\n",
      "1108/2595, train_loss: 0.2448\n",
      "1109/2595, train_loss: 0.5342\n",
      "1110/2595, train_loss: 0.3247\n",
      "1111/2595, train_loss: 0.1738\n",
      "1112/2595, train_loss: 0.0830\n",
      "1113/2595, train_loss: 0.3430\n",
      "1114/2595, train_loss: 0.4443\n",
      "1115/2595, train_loss: 0.7348\n",
      "1116/2595, train_loss: 0.6986\n",
      "1117/2595, train_loss: 0.2414\n",
      "1118/2595, train_loss: 0.4373\n",
      "1119/2595, train_loss: 0.4733\n",
      "1120/2595, train_loss: 0.1385\n",
      "1121/2595, train_loss: 0.9301\n",
      "1122/2595, train_loss: 0.1651\n",
      "1123/2595, train_loss: 0.1902\n",
      "1124/2595, train_loss: 0.4633\n",
      "1125/2595, train_loss: 0.2558\n",
      "1126/2595, train_loss: 0.5525\n",
      "1127/2595, train_loss: 0.3821\n",
      "1128/2595, train_loss: 0.3088\n",
      "1129/2595, train_loss: 0.3154\n",
      "1130/2595, train_loss: 0.2905\n",
      "1131/2595, train_loss: 0.3901\n",
      "1132/2595, train_loss: 0.1382\n",
      "1133/2595, train_loss: 0.3392\n",
      "1134/2595, train_loss: 0.1490\n",
      "1135/2595, train_loss: 0.5888\n",
      "1136/2595, train_loss: 0.5013\n",
      "1137/2595, train_loss: 0.1410\n",
      "1138/2595, train_loss: 0.3164\n",
      "1139/2595, train_loss: 0.5008\n",
      "1140/2595, train_loss: 0.3163\n",
      "1141/2595, train_loss: 0.2311\n",
      "1142/2595, train_loss: 0.3954\n",
      "1143/2595, train_loss: 0.2939\n",
      "1144/2595, train_loss: 0.2550\n",
      "1145/2595, train_loss: 0.5889\n",
      "1146/2595, train_loss: 0.2583\n",
      "1147/2595, train_loss: 0.1823\n",
      "1148/2595, train_loss: 0.4391\n",
      "1149/2595, train_loss: 0.1491\n",
      "1150/2595, train_loss: 0.2509\n",
      "1151/2595, train_loss: 0.5750\n",
      "1152/2595, train_loss: 0.4304\n",
      "1153/2595, train_loss: 0.6329\n",
      "1154/2595, train_loss: 0.3634\n",
      "1155/2595, train_loss: 0.4737\n",
      "1156/2595, train_loss: 0.2874\n",
      "1157/2595, train_loss: 0.1536\n",
      "1158/2595, train_loss: 0.7071\n",
      "1159/2595, train_loss: 0.4164\n",
      "1160/2595, train_loss: 0.7560\n",
      "1161/2595, train_loss: 0.4652\n",
      "1162/2595, train_loss: 0.3759\n",
      "1163/2595, train_loss: 0.5173\n",
      "1164/2595, train_loss: 0.3425\n",
      "1165/2595, train_loss: 0.5369\n",
      "1166/2595, train_loss: 0.1461\n",
      "1167/2595, train_loss: 0.4226\n",
      "1168/2595, train_loss: 0.2477\n",
      "1169/2595, train_loss: 0.2833\n",
      "1170/2595, train_loss: 0.2449\n",
      "1171/2595, train_loss: 0.2412\n",
      "1172/2595, train_loss: 0.3416\n",
      "1173/2595, train_loss: 0.2956\n",
      "1174/2595, train_loss: 0.2890\n",
      "1175/2595, train_loss: 0.2061\n",
      "1176/2595, train_loss: 0.5805\n",
      "1177/2595, train_loss: 0.2941\n",
      "1178/2595, train_loss: 0.2013\n",
      "1179/2595, train_loss: 0.2975\n",
      "1180/2595, train_loss: 0.2472\n",
      "1181/2595, train_loss: 0.2155\n",
      "1182/2595, train_loss: 0.2649\n",
      "1183/2595, train_loss: 0.3834\n",
      "1184/2595, train_loss: 0.1820\n",
      "1185/2595, train_loss: 0.1623\n",
      "1186/2595, train_loss: 0.0720\n",
      "1187/2595, train_loss: 0.6901\n",
      "1188/2595, train_loss: 0.4152\n",
      "1189/2595, train_loss: 0.3291\n",
      "1190/2595, train_loss: 0.4693\n",
      "1191/2595, train_loss: 0.3891\n",
      "1192/2595, train_loss: 0.2664\n",
      "1193/2595, train_loss: 0.5030\n",
      "1194/2595, train_loss: 0.7523\n",
      "1195/2595, train_loss: 0.3657\n",
      "1196/2595, train_loss: 0.4860\n",
      "1197/2595, train_loss: 0.1109\n",
      "1198/2595, train_loss: 0.4515\n",
      "1199/2595, train_loss: 0.3651\n",
      "1200/2595, train_loss: 0.4434\n",
      "1201/2595, train_loss: 0.4646\n",
      "1202/2595, train_loss: 0.4971\n",
      "1203/2595, train_loss: 0.6604\n",
      "1204/2595, train_loss: 0.1528\n",
      "1205/2595, train_loss: 0.7784\n",
      "1206/2595, train_loss: 0.1377\n",
      "1207/2595, train_loss: 0.1730\n",
      "1208/2595, train_loss: 0.1653\n",
      "1209/2595, train_loss: 0.4449\n",
      "1210/2595, train_loss: 0.2489\n",
      "1211/2595, train_loss: 0.5989\n",
      "1212/2595, train_loss: 0.1564\n",
      "1213/2595, train_loss: 0.2318\n",
      "1214/2595, train_loss: 0.3214\n",
      "1215/2595, train_loss: 0.7293\n",
      "1216/2595, train_loss: 0.1039\n",
      "1217/2595, train_loss: 0.5189\n",
      "1218/2595, train_loss: 0.2006\n",
      "1219/2595, train_loss: 0.1759\n",
      "1220/2595, train_loss: 0.4628\n",
      "1221/2595, train_loss: 0.1477\n",
      "1222/2595, train_loss: 0.2947\n",
      "1223/2595, train_loss: 0.1801\n",
      "1224/2595, train_loss: 0.1244\n",
      "1225/2595, train_loss: 0.2654\n",
      "1226/2595, train_loss: 0.3382\n",
      "1227/2595, train_loss: 0.3801\n",
      "1228/2595, train_loss: 0.4788\n",
      "1229/2595, train_loss: 0.1336\n",
      "1230/2595, train_loss: 0.3658\n",
      "1231/2595, train_loss: 0.1942\n",
      "1232/2595, train_loss: 0.1392\n",
      "1233/2595, train_loss: 0.1757\n",
      "1234/2595, train_loss: 0.1411\n",
      "1235/2595, train_loss: 0.5011\n",
      "1236/2595, train_loss: 0.1997\n",
      "1237/2595, train_loss: 0.5800\n",
      "1238/2595, train_loss: 0.7304\n",
      "1239/2595, train_loss: 0.3623\n",
      "1240/2595, train_loss: 0.1163\n",
      "1241/2595, train_loss: 0.1016\n",
      "1242/2595, train_loss: 0.7604\n",
      "1243/2595, train_loss: 0.1181\n",
      "1244/2595, train_loss: 0.1549\n",
      "1245/2595, train_loss: 0.7468\n",
      "1246/2595, train_loss: 0.3399\n",
      "1247/2595, train_loss: 0.1683\n",
      "1248/2595, train_loss: 0.6947\n",
      "1249/2595, train_loss: 0.4261\n",
      "1250/2595, train_loss: 0.1492\n",
      "1251/2595, train_loss: 1.4628\n",
      "1252/2595, train_loss: 0.3799\n",
      "1253/2595, train_loss: 0.2307\n",
      "1254/2595, train_loss: 0.1708\n",
      "1255/2595, train_loss: 0.6725\n",
      "1256/2595, train_loss: 0.4732\n",
      "1257/2595, train_loss: 0.2115\n",
      "1258/2595, train_loss: 0.3065\n",
      "1259/2595, train_loss: 0.1892\n",
      "1260/2595, train_loss: 0.5831\n",
      "1261/2595, train_loss: 0.2412\n",
      "1262/2595, train_loss: 0.1881\n",
      "1263/2595, train_loss: 0.2385\n",
      "1264/2595, train_loss: 0.3373\n",
      "1265/2595, train_loss: 0.2106\n",
      "1266/2595, train_loss: 0.2019\n",
      "1267/2595, train_loss: 0.3292\n",
      "1268/2595, train_loss: 0.1184\n",
      "1269/2595, train_loss: 0.2790\n",
      "1270/2595, train_loss: 0.3547\n",
      "1271/2595, train_loss: 0.4557\n",
      "1272/2595, train_loss: 0.2842\n",
      "1273/2595, train_loss: 0.1247\n",
      "1274/2595, train_loss: 0.2517\n",
      "1275/2595, train_loss: 0.1349\n",
      "1276/2595, train_loss: 0.5716\n",
      "1277/2595, train_loss: 0.3497\n",
      "1278/2595, train_loss: 0.5869\n",
      "1279/2595, train_loss: 0.7477\n",
      "1280/2595, train_loss: 0.3686\n",
      "1281/2595, train_loss: 0.1626\n",
      "1282/2595, train_loss: 0.5722\n",
      "1283/2595, train_loss: 0.1845\n",
      "1284/2595, train_loss: 0.7098\n",
      "1285/2595, train_loss: 0.1660\n",
      "1286/2595, train_loss: 0.6811\n",
      "1287/2595, train_loss: 0.5909\n",
      "1288/2595, train_loss: 0.6962\n",
      "1289/2595, train_loss: 0.2279\n",
      "1290/2595, train_loss: 0.3293\n",
      "1291/2595, train_loss: 0.4349\n",
      "1292/2595, train_loss: 0.9161\n",
      "1293/2595, train_loss: 0.3034\n",
      "1294/2595, train_loss: 0.5461\n",
      "1295/2595, train_loss: 0.2176\n",
      "1296/2595, train_loss: 0.3731\n",
      "1297/2595, train_loss: 0.5025\n",
      "1298/2595, train_loss: 0.2594\n",
      "1299/2595, train_loss: 0.4775\n",
      "1300/2595, train_loss: 0.2727\n",
      "1301/2595, train_loss: 0.2941\n",
      "1302/2595, train_loss: 0.1687\n",
      "1303/2595, train_loss: 0.4283\n",
      "1304/2595, train_loss: 0.2267\n",
      "1305/2595, train_loss: 0.1920\n",
      "1306/2595, train_loss: 0.2537\n",
      "1307/2595, train_loss: 0.2300\n",
      "1308/2595, train_loss: 0.4392\n",
      "1309/2595, train_loss: 0.2926\n",
      "1310/2595, train_loss: 0.1383\n",
      "1311/2595, train_loss: 0.3289\n",
      "1312/2595, train_loss: 0.7424\n",
      "1313/2595, train_loss: 0.4791\n",
      "1314/2595, train_loss: 0.0916\n",
      "1315/2595, train_loss: 0.1557\n",
      "1316/2595, train_loss: 0.7749\n",
      "1317/2595, train_loss: 0.1999\n",
      "1318/2595, train_loss: 0.3303\n",
      "1319/2595, train_loss: 0.1482\n",
      "1320/2595, train_loss: 0.1581\n",
      "1321/2595, train_loss: 0.2017\n",
      "1322/2595, train_loss: 0.6659\n",
      "1323/2595, train_loss: 0.1524\n",
      "1324/2595, train_loss: 0.3052\n",
      "1325/2595, train_loss: 0.3905\n",
      "1326/2595, train_loss: 0.3412\n",
      "1327/2595, train_loss: 0.0897\n",
      "1328/2595, train_loss: 0.6398\n",
      "1329/2595, train_loss: 0.3300\n",
      "1330/2595, train_loss: 0.2384\n",
      "1331/2595, train_loss: 0.3091\n",
      "1332/2595, train_loss: 0.1030\n",
      "1333/2595, train_loss: 0.2129\n",
      "1334/2595, train_loss: 0.3089\n",
      "1335/2595, train_loss: 0.2810\n",
      "1336/2595, train_loss: 0.2785\n",
      "1337/2595, train_loss: 0.0761\n",
      "1338/2595, train_loss: 0.6014\n",
      "1339/2595, train_loss: 0.6686\n",
      "1340/2595, train_loss: 0.5113\n",
      "1341/2595, train_loss: 0.4536\n",
      "1342/2595, train_loss: 0.8067\n",
      "1343/2595, train_loss: 0.1545\n",
      "1344/2595, train_loss: 0.1092\n",
      "1345/2595, train_loss: 0.4326\n",
      "1346/2595, train_loss: 0.3784\n",
      "1347/2595, train_loss: 0.7254\n",
      "1348/2595, train_loss: 0.1298\n",
      "1349/2595, train_loss: 0.1441\n",
      "1350/2595, train_loss: 0.1141\n",
      "1351/2595, train_loss: 0.2672\n",
      "1352/2595, train_loss: 0.3467\n",
      "1353/2595, train_loss: 0.0717\n",
      "1354/2595, train_loss: 0.4933\n",
      "1355/2595, train_loss: 0.5069\n",
      "1356/2595, train_loss: 0.4463\n",
      "1357/2595, train_loss: 0.7188\n",
      "1358/2595, train_loss: 0.2150\n",
      "1359/2595, train_loss: 0.1997\n",
      "1360/2595, train_loss: 0.2153\n",
      "1361/2595, train_loss: 0.3030\n",
      "1362/2595, train_loss: 0.2092\n",
      "1363/2595, train_loss: 0.5194\n",
      "1364/2595, train_loss: 0.5643\n",
      "1365/2595, train_loss: 0.4464\n",
      "1366/2595, train_loss: 0.3378\n",
      "1367/2595, train_loss: 0.3800\n",
      "1368/2595, train_loss: 0.3076\n",
      "1369/2595, train_loss: 0.3855\n",
      "1370/2595, train_loss: 0.4150\n",
      "1371/2595, train_loss: 0.2084\n",
      "1372/2595, train_loss: 0.3029\n",
      "1373/2595, train_loss: 0.4392\n",
      "1374/2595, train_loss: 0.3091\n",
      "1375/2595, train_loss: 0.3744\n",
      "1376/2595, train_loss: 0.1760\n",
      "1377/2595, train_loss: 0.8243\n",
      "1378/2595, train_loss: 0.2631\n",
      "1379/2595, train_loss: 0.6772\n",
      "1380/2595, train_loss: 0.1058\n",
      "1381/2595, train_loss: 0.0969\n",
      "1382/2595, train_loss: 0.2227\n",
      "1383/2595, train_loss: 0.3030\n",
      "1384/2595, train_loss: 0.3197\n",
      "1385/2595, train_loss: 0.0660\n",
      "1386/2595, train_loss: 0.0819\n",
      "1387/2595, train_loss: 0.4194\n",
      "1388/2595, train_loss: 0.2275\n",
      "1389/2595, train_loss: 0.1767\n",
      "1390/2595, train_loss: 0.2888\n",
      "1391/2595, train_loss: 0.2580\n",
      "1392/2595, train_loss: 0.3764\n",
      "1393/2595, train_loss: 0.2260\n",
      "1394/2595, train_loss: 0.1777\n",
      "1395/2595, train_loss: 0.3318\n",
      "1396/2595, train_loss: 0.4908\n",
      "1397/2595, train_loss: 0.5630\n",
      "1398/2595, train_loss: 0.2549\n",
      "1399/2595, train_loss: 0.1083\n",
      "1400/2595, train_loss: 0.3257\n",
      "1401/2595, train_loss: 0.1203\n",
      "1402/2595, train_loss: 0.5763\n",
      "1403/2595, train_loss: 0.5098\n",
      "1404/2595, train_loss: 0.0778\n",
      "1405/2595, train_loss: 0.1606\n",
      "1406/2595, train_loss: 0.2027\n",
      "1407/2595, train_loss: 0.1279\n",
      "1408/2595, train_loss: 0.5652\n",
      "1409/2595, train_loss: 1.1520\n",
      "1410/2595, train_loss: 0.3183\n",
      "1411/2595, train_loss: 0.3452\n",
      "1412/2595, train_loss: 0.2805\n",
      "1413/2595, train_loss: 0.4757\n",
      "1414/2595, train_loss: 0.6340\n",
      "1415/2595, train_loss: 0.5092\n",
      "1416/2595, train_loss: 0.4065\n",
      "1417/2595, train_loss: 0.7543\n",
      "1418/2595, train_loss: 0.1125\n",
      "1419/2595, train_loss: 0.1353\n",
      "1420/2595, train_loss: 0.3156\n",
      "1421/2595, train_loss: 0.1714\n",
      "1422/2595, train_loss: 0.2318\n",
      "1423/2595, train_loss: 0.1709\n",
      "1424/2595, train_loss: 0.2692\n",
      "1425/2595, train_loss: 0.2802\n",
      "1426/2595, train_loss: 0.4062\n",
      "1427/2595, train_loss: 0.4172\n",
      "1428/2595, train_loss: 0.5212\n",
      "1429/2595, train_loss: 0.2022\n",
      "1430/2595, train_loss: 0.2283\n",
      "1431/2595, train_loss: 0.7112\n",
      "1432/2595, train_loss: 0.6741\n",
      "1433/2595, train_loss: 0.3747\n",
      "1434/2595, train_loss: 0.1058\n",
      "1435/2595, train_loss: 0.2000\n",
      "1436/2595, train_loss: 0.2190\n",
      "1437/2595, train_loss: 0.1661\n",
      "1438/2595, train_loss: 0.6186\n",
      "1439/2595, train_loss: 0.1990\n",
      "1440/2595, train_loss: 0.4176\n",
      "1441/2595, train_loss: 0.1862\n",
      "1442/2595, train_loss: 0.7042\n",
      "1443/2595, train_loss: 0.4920\n",
      "1444/2595, train_loss: 0.2187\n",
      "1445/2595, train_loss: 0.1016\n",
      "1446/2595, train_loss: 0.7714\n",
      "1447/2595, train_loss: 0.1689\n",
      "1448/2595, train_loss: 0.2498\n",
      "1449/2595, train_loss: 0.2387\n",
      "1450/2595, train_loss: 0.1653\n",
      "1451/2595, train_loss: 0.3357\n",
      "1452/2595, train_loss: 0.4696\n",
      "1453/2595, train_loss: 1.0951\n",
      "1454/2595, train_loss: 0.5048\n",
      "1455/2595, train_loss: 0.2243\n",
      "1456/2595, train_loss: 0.3151\n",
      "1457/2595, train_loss: 0.0995\n",
      "1458/2595, train_loss: 0.2453\n",
      "1459/2595, train_loss: 0.1694\n",
      "1460/2595, train_loss: 0.3787\n",
      "1461/2595, train_loss: 0.1572\n",
      "1462/2595, train_loss: 0.2819\n",
      "1463/2595, train_loss: 0.4893\n",
      "1464/2595, train_loss: 0.2872\n",
      "1465/2595, train_loss: 0.6523\n",
      "1466/2595, train_loss: 0.5965\n",
      "1467/2595, train_loss: 0.5260\n",
      "1468/2595, train_loss: 0.2354\n",
      "1469/2595, train_loss: 0.3728\n",
      "1470/2595, train_loss: 0.3002\n",
      "1471/2595, train_loss: 0.5380\n",
      "1472/2595, train_loss: 0.2390\n",
      "1473/2595, train_loss: 0.4958\n",
      "1474/2595, train_loss: 0.2384\n",
      "1475/2595, train_loss: 0.1837\n",
      "1476/2595, train_loss: 0.5584\n",
      "1477/2595, train_loss: 0.1330\n",
      "1478/2595, train_loss: 0.1054\n",
      "1479/2595, train_loss: 0.5808\n",
      "1480/2595, train_loss: 0.4500\n",
      "1481/2595, train_loss: 0.5567\n",
      "1482/2595, train_loss: 0.4782\n",
      "1483/2595, train_loss: 0.2460\n",
      "1484/2595, train_loss: 0.4774\n",
      "1485/2595, train_loss: 0.1276\n",
      "1486/2595, train_loss: 0.4419\n",
      "1487/2595, train_loss: 0.1511\n",
      "1488/2595, train_loss: 0.3834\n",
      "1489/2595, train_loss: 0.2351\n",
      "1490/2595, train_loss: 0.3337\n",
      "1491/2595, train_loss: 0.4117\n",
      "1492/2595, train_loss: 0.3588\n",
      "1493/2595, train_loss: 0.2806\n",
      "1494/2595, train_loss: 0.3520\n",
      "1495/2595, train_loss: 0.4929\n",
      "1496/2595, train_loss: 0.4277\n",
      "1497/2595, train_loss: 0.1551\n",
      "1498/2595, train_loss: 0.3080\n",
      "1499/2595, train_loss: 0.4528\n",
      "1500/2595, train_loss: 0.3515\n",
      "1501/2595, train_loss: 0.2772\n",
      "1502/2595, train_loss: 0.2048\n",
      "1503/2595, train_loss: 0.6572\n",
      "1504/2595, train_loss: 0.4844\n",
      "1505/2595, train_loss: 0.3845\n",
      "1506/2595, train_loss: 0.2151\n",
      "1507/2595, train_loss: 0.3575\n",
      "1508/2595, train_loss: 0.1126\n",
      "1509/2595, train_loss: 0.6224\n",
      "1510/2595, train_loss: 0.2042\n",
      "1511/2595, train_loss: 0.5342\n",
      "1512/2595, train_loss: 0.3646\n",
      "1513/2595, train_loss: 0.1377\n",
      "1514/2595, train_loss: 0.2851\n",
      "1515/2595, train_loss: 0.6597\n",
      "1516/2595, train_loss: 0.1139\n",
      "1517/2595, train_loss: 0.4890\n",
      "1518/2595, train_loss: 0.4268\n",
      "1519/2595, train_loss: 0.6032\n",
      "1520/2595, train_loss: 0.2455\n",
      "1521/2595, train_loss: 0.2445\n",
      "1522/2595, train_loss: 0.4285\n",
      "1523/2595, train_loss: 0.3343\n",
      "1524/2595, train_loss: 0.3355\n",
      "1525/2595, train_loss: 0.3512\n",
      "1526/2595, train_loss: 0.2053\n",
      "1527/2595, train_loss: 0.4618\n",
      "1528/2595, train_loss: 0.2966\n",
      "1529/2595, train_loss: 0.2283\n",
      "1530/2595, train_loss: 0.3052\n",
      "1531/2595, train_loss: 0.3464\n",
      "1532/2595, train_loss: 0.3682\n",
      "1533/2595, train_loss: 0.0889\n",
      "1534/2595, train_loss: 0.3281\n",
      "1535/2595, train_loss: 0.3013\n",
      "1536/2595, train_loss: 0.3345\n",
      "1537/2595, train_loss: 0.3745\n",
      "1538/2595, train_loss: 0.5047\n",
      "1539/2595, train_loss: 0.1844\n",
      "1540/2595, train_loss: 0.3862\n",
      "1541/2595, train_loss: 0.2644\n",
      "1542/2595, train_loss: 0.3119\n",
      "1543/2595, train_loss: 0.7295\n",
      "1544/2595, train_loss: 0.4728\n",
      "1545/2595, train_loss: 0.2351\n",
      "1546/2595, train_loss: 0.1617\n",
      "1547/2595, train_loss: 0.2894\n",
      "1548/2595, train_loss: 0.1619\n",
      "1549/2595, train_loss: 0.3423\n",
      "1550/2595, train_loss: 0.2532\n",
      "1551/2595, train_loss: 0.2871\n",
      "1552/2595, train_loss: 0.4068\n",
      "1553/2595, train_loss: 0.2568\n",
      "1554/2595, train_loss: 0.4411\n",
      "1555/2595, train_loss: 0.3612\n",
      "1556/2595, train_loss: 0.1782\n",
      "1557/2595, train_loss: 0.1646\n",
      "1558/2595, train_loss: 0.2998\n",
      "1559/2595, train_loss: 0.1440\n",
      "1560/2595, train_loss: 0.4857\n",
      "1561/2595, train_loss: 0.4238\n",
      "1562/2595, train_loss: 0.2433\n",
      "1563/2595, train_loss: 0.1590\n",
      "1564/2595, train_loss: 0.6897\n",
      "1565/2595, train_loss: 0.2414\n",
      "1566/2595, train_loss: 0.1393\n",
      "1567/2595, train_loss: 0.2111\n",
      "1568/2595, train_loss: 0.5378\n",
      "1569/2595, train_loss: 0.5584\n",
      "1570/2595, train_loss: 0.1394\n",
      "1571/2595, train_loss: 0.3683\n",
      "1572/2595, train_loss: 0.3937\n",
      "1573/2595, train_loss: 0.2122\n",
      "1574/2595, train_loss: 0.1593\n",
      "1575/2595, train_loss: 0.2515\n",
      "1576/2595, train_loss: 0.1238\n",
      "1577/2595, train_loss: 0.1943\n",
      "1578/2595, train_loss: 0.1134\n",
      "1579/2595, train_loss: 0.2601\n",
      "1580/2595, train_loss: 0.3009\n",
      "1581/2595, train_loss: 0.1178\n",
      "1582/2595, train_loss: 0.7873\n",
      "1583/2595, train_loss: 0.0855\n",
      "1584/2595, train_loss: 0.1717\n",
      "1585/2595, train_loss: 0.1809\n",
      "1586/2595, train_loss: 0.4253\n",
      "1587/2595, train_loss: 0.1082\n",
      "1588/2595, train_loss: 0.3262\n",
      "1589/2595, train_loss: 0.1324\n",
      "1590/2595, train_loss: 0.5235\n",
      "1591/2595, train_loss: 0.3634\n",
      "1592/2595, train_loss: 0.0485\n",
      "1593/2595, train_loss: 0.3696\n",
      "1594/2595, train_loss: 0.3653\n",
      "1595/2595, train_loss: 0.1307\n",
      "1596/2595, train_loss: 0.0862\n",
      "1597/2595, train_loss: 0.3364\n",
      "1598/2595, train_loss: 0.2188\n",
      "1599/2595, train_loss: 0.2965\n",
      "1600/2595, train_loss: 0.0780\n",
      "1601/2595, train_loss: 0.1433\n",
      "1602/2595, train_loss: 0.1052\n",
      "1603/2595, train_loss: 0.0769\n",
      "1604/2595, train_loss: 0.1704\n",
      "1605/2595, train_loss: 0.1313\n",
      "1606/2595, train_loss: 0.1385\n",
      "1607/2595, train_loss: 0.2214\n",
      "1608/2595, train_loss: 0.6453\n",
      "1609/2595, train_loss: 0.8373\n",
      "1610/2595, train_loss: 0.3139\n",
      "1611/2595, train_loss: 0.3801\n",
      "1612/2595, train_loss: 0.1254\n",
      "1613/2595, train_loss: 0.2556\n",
      "1614/2595, train_loss: 0.0886\n",
      "1615/2595, train_loss: 0.8631\n",
      "1616/2595, train_loss: 0.0936\n",
      "1617/2595, train_loss: 0.2689\n",
      "1618/2595, train_loss: 0.4148\n",
      "1619/2595, train_loss: 0.1695\n",
      "1620/2595, train_loss: 0.0933\n",
      "1621/2595, train_loss: 0.4333\n",
      "1622/2595, train_loss: 0.1068\n",
      "1623/2595, train_loss: 0.9389\n",
      "1624/2595, train_loss: 0.2812\n",
      "1625/2595, train_loss: 0.1951\n",
      "1626/2595, train_loss: 0.1962\n",
      "1627/2595, train_loss: 0.3836\n",
      "1628/2595, train_loss: 0.2558\n",
      "1629/2595, train_loss: 0.1062\n",
      "1630/2595, train_loss: 0.3520\n",
      "1631/2595, train_loss: 0.4504\n",
      "1632/2595, train_loss: 0.0763\n",
      "1633/2595, train_loss: 0.2391\n",
      "1634/2595, train_loss: 0.1044\n",
      "1635/2595, train_loss: 0.2577\n",
      "1636/2595, train_loss: 0.2079\n",
      "1637/2595, train_loss: 1.0508\n",
      "1638/2595, train_loss: 0.3690\n",
      "1639/2595, train_loss: 0.2209\n",
      "1640/2595, train_loss: 0.1270\n",
      "1641/2595, train_loss: 0.5918\n",
      "1642/2595, train_loss: 0.2830\n",
      "1643/2595, train_loss: 0.6194\n",
      "1644/2595, train_loss: 0.2609\n",
      "1645/2595, train_loss: 0.3079\n",
      "1646/2595, train_loss: 0.2606\n",
      "1647/2595, train_loss: 0.3872\n",
      "1648/2595, train_loss: 0.4689\n",
      "1649/2595, train_loss: 0.4327\n",
      "1650/2595, train_loss: 0.4240\n",
      "1651/2595, train_loss: 0.1294\n",
      "1652/2595, train_loss: 0.5378\n",
      "1653/2595, train_loss: 0.3594\n",
      "1654/2595, train_loss: 0.2901\n",
      "1655/2595, train_loss: 0.6268\n",
      "1656/2595, train_loss: 0.1768\n",
      "1657/2595, train_loss: 0.4194\n",
      "1658/2595, train_loss: 0.4285\n",
      "1659/2595, train_loss: 0.3464\n",
      "1660/2595, train_loss: 0.4383\n",
      "1661/2595, train_loss: 0.2039\n",
      "1662/2595, train_loss: 0.4925\n",
      "1663/2595, train_loss: 0.7536\n",
      "1664/2595, train_loss: 0.2745\n",
      "1665/2595, train_loss: 0.2274\n",
      "1666/2595, train_loss: 0.3137\n",
      "1667/2595, train_loss: 0.2281\n",
      "1668/2595, train_loss: 0.3126\n",
      "1669/2595, train_loss: 0.1507\n",
      "1670/2595, train_loss: 0.1683\n",
      "1671/2595, train_loss: 0.2500\n",
      "1672/2595, train_loss: 0.1002\n",
      "1673/2595, train_loss: 0.1981\n",
      "1674/2595, train_loss: 0.3934\n",
      "1675/2595, train_loss: 0.3133\n",
      "1676/2595, train_loss: 0.2268\n",
      "1677/2595, train_loss: 0.4983\n",
      "1678/2595, train_loss: 0.2103\n",
      "1679/2595, train_loss: 0.1445\n",
      "1680/2595, train_loss: 0.1113\n",
      "1681/2595, train_loss: 0.4567\n",
      "1682/2595, train_loss: 0.5075\n",
      "1683/2595, train_loss: 0.5083\n",
      "1684/2595, train_loss: 0.3583\n",
      "1685/2595, train_loss: 0.1193\n",
      "1686/2595, train_loss: 0.6402\n",
      "1687/2595, train_loss: 0.3990\n",
      "1688/2595, train_loss: 0.1589\n",
      "1689/2595, train_loss: 0.3683\n",
      "1690/2595, train_loss: 0.1162\n",
      "1691/2595, train_loss: 0.4466\n",
      "1692/2595, train_loss: 0.7007\n",
      "1693/2595, train_loss: 0.3191\n",
      "1694/2595, train_loss: 0.5254\n",
      "1695/2595, train_loss: 0.3491\n",
      "1696/2595, train_loss: 0.2018\n",
      "1697/2595, train_loss: 0.1020\n",
      "1698/2595, train_loss: 0.2932\n",
      "1699/2595, train_loss: 0.5377\n",
      "1700/2595, train_loss: 0.1140\n",
      "1701/2595, train_loss: 0.4469\n",
      "1702/2595, train_loss: 0.6655\n",
      "1703/2595, train_loss: 0.2719\n",
      "1704/2595, train_loss: 0.1432\n",
      "1705/2595, train_loss: 0.1703\n",
      "1706/2595, train_loss: 0.2867\n",
      "1707/2595, train_loss: 0.1671\n",
      "1708/2595, train_loss: 0.1165\n",
      "1709/2595, train_loss: 0.5011\n",
      "1710/2595, train_loss: 0.0731\n",
      "1711/2595, train_loss: 0.1205\n",
      "1712/2595, train_loss: 0.3774\n",
      "1713/2595, train_loss: 0.4690\n",
      "1714/2595, train_loss: 0.1562\n",
      "1715/2595, train_loss: 0.1600\n",
      "1716/2595, train_loss: 0.1026\n",
      "1717/2595, train_loss: 0.5266\n",
      "1718/2595, train_loss: 0.3986\n",
      "1719/2595, train_loss: 0.5618\n",
      "1720/2595, train_loss: 0.0804\n",
      "1721/2595, train_loss: 0.3728\n",
      "1722/2595, train_loss: 0.0832\n",
      "1723/2595, train_loss: 0.0779\n",
      "1724/2595, train_loss: 0.5137\n",
      "1725/2595, train_loss: 0.2995\n",
      "1726/2595, train_loss: 0.2445\n",
      "1727/2595, train_loss: 0.1840\n",
      "1728/2595, train_loss: 0.1144\n",
      "1729/2595, train_loss: 0.2007\n",
      "1730/2595, train_loss: 0.3456\n",
      "1731/2595, train_loss: 0.4311\n",
      "1732/2595, train_loss: 0.7064\n",
      "1733/2595, train_loss: 0.3245\n",
      "1734/2595, train_loss: 0.1379\n",
      "1735/2595, train_loss: 0.5418\n",
      "1736/2595, train_loss: 0.3068\n",
      "1737/2595, train_loss: 0.3970\n",
      "1738/2595, train_loss: 0.2465\n",
      "1739/2595, train_loss: 0.1508\n",
      "1740/2595, train_loss: 0.4661\n",
      "1741/2595, train_loss: 0.6226\n",
      "1742/2595, train_loss: 0.3689\n",
      "1743/2595, train_loss: 0.2061\n",
      "1744/2595, train_loss: 0.3105\n",
      "1745/2595, train_loss: 0.1564\n",
      "1746/2595, train_loss: 0.4456\n",
      "1747/2595, train_loss: 0.2996\n",
      "1748/2595, train_loss: 0.8013\n",
      "1749/2595, train_loss: 0.2024\n",
      "1750/2595, train_loss: 0.1606\n",
      "1751/2595, train_loss: 0.1469\n",
      "1752/2595, train_loss: 0.3977\n",
      "1753/2595, train_loss: 0.4486\n",
      "1754/2595, train_loss: 0.2409\n",
      "1755/2595, train_loss: 0.5017\n",
      "1756/2595, train_loss: 0.2413\n",
      "1757/2595, train_loss: 0.4102\n",
      "1758/2595, train_loss: 0.4253\n",
      "1759/2595, train_loss: 0.3608\n",
      "1760/2595, train_loss: 0.1465\n",
      "1761/2595, train_loss: 0.2443\n",
      "1762/2595, train_loss: 0.3696\n",
      "1763/2595, train_loss: 0.4404\n",
      "1764/2595, train_loss: 0.2672\n",
      "1765/2595, train_loss: 0.1614\n",
      "1766/2595, train_loss: 0.1770\n",
      "1767/2595, train_loss: 0.0937\n",
      "1768/2595, train_loss: 0.6856\n",
      "1769/2595, train_loss: 0.5634\n",
      "1770/2595, train_loss: 0.8075\n",
      "1771/2595, train_loss: 0.1561\n",
      "1772/2595, train_loss: 0.3166\n",
      "1773/2595, train_loss: 0.1468\n",
      "1774/2595, train_loss: 1.3112\n",
      "1775/2595, train_loss: 0.6647\n",
      "1776/2595, train_loss: 0.2217\n",
      "1777/2595, train_loss: 0.2973\n",
      "1778/2595, train_loss: 0.2502\n",
      "1779/2595, train_loss: 0.6200\n",
      "1780/2595, train_loss: 0.4533\n",
      "1781/2595, train_loss: 0.3142\n",
      "1782/2595, train_loss: 0.7020\n",
      "1783/2595, train_loss: 0.1484\n",
      "1784/2595, train_loss: 0.3144\n",
      "1785/2595, train_loss: 0.4003\n",
      "1786/2595, train_loss: 0.4829\n",
      "1787/2595, train_loss: 0.3606\n",
      "1788/2595, train_loss: 0.1895\n",
      "1789/2595, train_loss: 0.1689\n",
      "1790/2595, train_loss: 0.2262\n",
      "1791/2595, train_loss: 0.4632\n",
      "1792/2595, train_loss: 0.2676\n",
      "1793/2595, train_loss: 0.4145\n",
      "1794/2595, train_loss: 0.3729\n",
      "1795/2595, train_loss: 0.1935\n",
      "1796/2595, train_loss: 0.3820\n",
      "1797/2595, train_loss: 0.1139\n",
      "1798/2595, train_loss: 0.1945\n",
      "1799/2595, train_loss: 0.3487\n",
      "1800/2595, train_loss: 0.6030\n",
      "1801/2595, train_loss: 0.8186\n",
      "1802/2595, train_loss: 0.1313\n",
      "1803/2595, train_loss: 0.2679\n",
      "1804/2595, train_loss: 0.3814\n",
      "1805/2595, train_loss: 0.1921\n",
      "1806/2595, train_loss: 0.2126\n",
      "1807/2595, train_loss: 0.0805\n",
      "1808/2595, train_loss: 0.1427\n",
      "1809/2595, train_loss: 0.2323\n",
      "1810/2595, train_loss: 0.2608\n",
      "1811/2595, train_loss: 0.3092\n",
      "1812/2595, train_loss: 0.2978\n",
      "1813/2595, train_loss: 0.1217\n",
      "1814/2595, train_loss: 0.1286\n",
      "1815/2595, train_loss: 0.1347\n",
      "1816/2595, train_loss: 0.3876\n",
      "1817/2595, train_loss: 0.4410\n",
      "1818/2595, train_loss: 0.3547\n",
      "1819/2595, train_loss: 0.0642\n",
      "1820/2595, train_loss: 0.2678\n",
      "1821/2595, train_loss: 0.4274\n",
      "1822/2595, train_loss: 0.5426\n",
      "1823/2595, train_loss: 0.1273\n",
      "1824/2595, train_loss: 0.1897\n",
      "1825/2595, train_loss: 0.2083\n",
      "1826/2595, train_loss: 0.4101\n",
      "1827/2595, train_loss: 0.6463\n",
      "1828/2595, train_loss: 0.2980\n",
      "1829/2595, train_loss: 0.4359\n",
      "1830/2595, train_loss: 0.2213\n",
      "1831/2595, train_loss: 0.0799\n",
      "1832/2595, train_loss: 0.5609\n",
      "1833/2595, train_loss: 0.2371\n",
      "1834/2595, train_loss: 0.2751\n",
      "1835/2595, train_loss: 0.5303\n",
      "1836/2595, train_loss: 0.5498\n",
      "1837/2595, train_loss: 0.0672\n",
      "1838/2595, train_loss: 0.3245\n",
      "1839/2595, train_loss: 0.4144\n",
      "1840/2595, train_loss: 0.0849\n",
      "1841/2595, train_loss: 0.2464\n",
      "1842/2595, train_loss: 0.4171\n",
      "1843/2595, train_loss: 0.2872\n",
      "1844/2595, train_loss: 0.1546\n",
      "1845/2595, train_loss: 0.0453\n",
      "1846/2595, train_loss: 0.6778\n",
      "1847/2595, train_loss: 0.0784\n",
      "1848/2595, train_loss: 0.0681\n",
      "1849/2595, train_loss: 0.4934\n",
      "1850/2595, train_loss: 0.4552\n",
      "1851/2595, train_loss: 0.2465\n",
      "1852/2595, train_loss: 0.3499\n",
      "1853/2595, train_loss: 0.4867\n",
      "1854/2595, train_loss: 0.3007\n",
      "1855/2595, train_loss: 0.3005\n",
      "1856/2595, train_loss: 0.3673\n",
      "1857/2595, train_loss: 0.3191\n",
      "1858/2595, train_loss: 0.1987\n",
      "1859/2595, train_loss: 0.1628\n",
      "1860/2595, train_loss: 0.3369\n",
      "1861/2595, train_loss: 0.2509\n",
      "1862/2595, train_loss: 0.5887\n",
      "1863/2595, train_loss: 0.2918\n",
      "1864/2595, train_loss: 0.4538\n",
      "1865/2595, train_loss: 0.3546\n",
      "1866/2595, train_loss: 0.1340\n",
      "1867/2595, train_loss: 0.0857\n",
      "1868/2595, train_loss: 0.4023\n",
      "1869/2595, train_loss: 0.6315\n",
      "1870/2595, train_loss: 0.4763\n",
      "1871/2595, train_loss: 0.4548\n",
      "1872/2595, train_loss: 0.9694\n",
      "1873/2595, train_loss: 0.5843\n",
      "1874/2595, train_loss: 0.2824\n",
      "1875/2595, train_loss: 0.1059\n",
      "1876/2595, train_loss: 0.3599\n",
      "1877/2595, train_loss: 0.4711\n",
      "1878/2595, train_loss: 0.4031\n",
      "1879/2595, train_loss: 0.4537\n",
      "1880/2595, train_loss: 0.9231\n",
      "1881/2595, train_loss: 0.2785\n",
      "1882/2595, train_loss: 0.6332\n",
      "1883/2595, train_loss: 0.2653\n",
      "1884/2595, train_loss: 0.1679\n",
      "1885/2595, train_loss: 0.3484\n",
      "1886/2595, train_loss: 0.3472\n",
      "1887/2595, train_loss: 0.2151\n",
      "1888/2595, train_loss: 0.2673\n",
      "1889/2595, train_loss: 0.5135\n",
      "1890/2595, train_loss: 0.2539\n",
      "1891/2595, train_loss: 0.2861\n",
      "1892/2595, train_loss: 0.5684\n",
      "1893/2595, train_loss: 0.2746\n",
      "1894/2595, train_loss: 0.5685\n",
      "1895/2595, train_loss: 0.1138\n",
      "1896/2595, train_loss: 0.3003\n",
      "1897/2595, train_loss: 0.5453\n",
      "1898/2595, train_loss: 0.1432\n",
      "1899/2595, train_loss: 0.2660\n",
      "1900/2595, train_loss: 0.3676\n",
      "1901/2595, train_loss: 0.4101\n",
      "1902/2595, train_loss: 0.1713\n",
      "1903/2595, train_loss: 0.4461\n",
      "1904/2595, train_loss: 0.1774\n",
      "1905/2595, train_loss: 0.2531\n",
      "1906/2595, train_loss: 0.3091\n",
      "1907/2595, train_loss: 0.1865\n",
      "1908/2595, train_loss: 0.4636\n",
      "1909/2595, train_loss: 0.1300\n",
      "1910/2595, train_loss: 0.3939\n",
      "1911/2595, train_loss: 0.1851\n",
      "1912/2595, train_loss: 0.3802\n",
      "1913/2595, train_loss: 0.0900\n",
      "1914/2595, train_loss: 0.3191\n",
      "1915/2595, train_loss: 0.1182\n",
      "1916/2595, train_loss: 0.3692\n",
      "1917/2595, train_loss: 0.1974\n",
      "1918/2595, train_loss: 0.3630\n",
      "1919/2595, train_loss: 0.1926\n",
      "1920/2595, train_loss: 0.7677\n",
      "1921/2595, train_loss: 0.2287\n",
      "1922/2595, train_loss: 0.2530\n",
      "1923/2595, train_loss: 0.2287\n",
      "1924/2595, train_loss: 0.2968\n",
      "1925/2595, train_loss: 0.7271\n",
      "1926/2595, train_loss: 0.4675\n",
      "1927/2595, train_loss: 0.3228\n",
      "1928/2595, train_loss: 0.1331\n",
      "1929/2595, train_loss: 0.1044\n",
      "1930/2595, train_loss: 0.4010\n",
      "1931/2595, train_loss: 0.2995\n",
      "1932/2595, train_loss: 0.3388\n",
      "1933/2595, train_loss: 0.1637\n",
      "1934/2595, train_loss: 0.1991\n",
      "1935/2595, train_loss: 0.1446\n",
      "1936/2595, train_loss: 0.3241\n",
      "1937/2595, train_loss: 0.4447\n",
      "1938/2595, train_loss: 0.1340\n",
      "1939/2595, train_loss: 1.0994\n",
      "1940/2595, train_loss: 0.1934\n",
      "1941/2595, train_loss: 0.2150\n",
      "1942/2595, train_loss: 0.2976\n",
      "1943/2595, train_loss: 0.2738\n",
      "1944/2595, train_loss: 0.1528\n",
      "1945/2595, train_loss: 0.1289\n",
      "1946/2595, train_loss: 0.1802\n",
      "1947/2595, train_loss: 0.0827\n",
      "1948/2595, train_loss: 0.2313\n",
      "1949/2595, train_loss: 0.1872\n",
      "1950/2595, train_loss: 0.1216\n",
      "1951/2595, train_loss: 0.1935\n",
      "1952/2595, train_loss: 0.2902\n",
      "1953/2595, train_loss: 0.2361\n",
      "1954/2595, train_loss: 0.1898\n",
      "1955/2595, train_loss: 0.3005\n",
      "1956/2595, train_loss: 0.4105\n",
      "1957/2595, train_loss: 0.0761\n",
      "1958/2595, train_loss: 0.4743\n",
      "1959/2595, train_loss: 0.2210\n",
      "1960/2595, train_loss: 0.3914\n",
      "1961/2595, train_loss: 0.4484\n",
      "1962/2595, train_loss: 0.4035\n",
      "1963/2595, train_loss: 0.3827\n",
      "1964/2595, train_loss: 0.1582\n",
      "1965/2595, train_loss: 0.3929\n",
      "1966/2595, train_loss: 0.2348\n",
      "1967/2595, train_loss: 0.3350\n",
      "1968/2595, train_loss: 0.3597\n",
      "1969/2595, train_loss: 0.4125\n",
      "1970/2595, train_loss: 0.4526\n",
      "1971/2595, train_loss: 0.1391\n",
      "1972/2595, train_loss: 0.2976\n",
      "1973/2595, train_loss: 0.5419\n",
      "1974/2595, train_loss: 0.3643\n",
      "1975/2595, train_loss: 0.2214\n",
      "1976/2595, train_loss: 0.5237\n",
      "1977/2595, train_loss: 0.2211\n",
      "1978/2595, train_loss: 0.2868\n",
      "1979/2595, train_loss: 0.3145\n",
      "1980/2595, train_loss: 0.1659\n",
      "1981/2595, train_loss: 0.5394\n",
      "1982/2595, train_loss: 0.2585\n",
      "1983/2595, train_loss: 0.6962\n",
      "1984/2595, train_loss: 0.3501\n",
      "1985/2595, train_loss: 0.1927\n",
      "1986/2595, train_loss: 0.2348\n",
      "1987/2595, train_loss: 0.4900\n",
      "1988/2595, train_loss: 0.2639\n",
      "1989/2595, train_loss: 0.2657\n",
      "1990/2595, train_loss: 0.4166\n",
      "1991/2595, train_loss: 0.1934\n",
      "1992/2595, train_loss: 0.4730\n",
      "1993/2595, train_loss: 0.3121\n",
      "1994/2595, train_loss: 0.3937\n",
      "1995/2595, train_loss: 0.1875\n",
      "1996/2595, train_loss: 0.2457\n",
      "1997/2595, train_loss: 0.2218\n",
      "1998/2595, train_loss: 0.4240\n",
      "1999/2595, train_loss: 0.3172\n",
      "2000/2595, train_loss: 0.3070\n",
      "2001/2595, train_loss: 0.5948\n",
      "2002/2595, train_loss: 0.1668\n",
      "2003/2595, train_loss: 0.4212\n",
      "2004/2595, train_loss: 0.4708\n",
      "2005/2595, train_loss: 0.3197\n",
      "2006/2595, train_loss: 0.1449\n",
      "2007/2595, train_loss: 0.2362\n",
      "2008/2595, train_loss: 0.4197\n",
      "2009/2595, train_loss: 0.7970\n",
      "2010/2595, train_loss: 0.2654\n",
      "2011/2595, train_loss: 0.2969\n",
      "2012/2595, train_loss: 0.6186\n",
      "2013/2595, train_loss: 0.2769\n",
      "2014/2595, train_loss: 0.1125\n",
      "2015/2595, train_loss: 0.2439\n",
      "2016/2595, train_loss: 1.2485\n",
      "2017/2595, train_loss: 0.4690\n",
      "2018/2595, train_loss: 0.2252\n",
      "2019/2595, train_loss: 0.4470\n",
      "2020/2595, train_loss: 0.5471\n",
      "2021/2595, train_loss: 0.0999\n",
      "2022/2595, train_loss: 0.2872\n",
      "2023/2595, train_loss: 0.6543\n",
      "2024/2595, train_loss: 0.2833\n",
      "2025/2595, train_loss: 0.2334\n",
      "2026/2595, train_loss: 0.5549\n",
      "2027/2595, train_loss: 0.2564\n",
      "2028/2595, train_loss: 0.1118\n",
      "2029/2595, train_loss: 0.4812\n",
      "2030/2595, train_loss: 0.0663\n",
      "2031/2595, train_loss: 0.1776\n",
      "2032/2595, train_loss: 0.2282\n",
      "2033/2595, train_loss: 0.0514\n",
      "2034/2595, train_loss: 0.1336\n",
      "2035/2595, train_loss: 0.1583\n",
      "2036/2595, train_loss: 0.2540\n",
      "2037/2595, train_loss: 1.0150\n",
      "2038/2595, train_loss: 0.4794\n",
      "2039/2595, train_loss: 0.1502\n",
      "2040/2595, train_loss: 0.3919\n",
      "2041/2595, train_loss: 0.6977\n",
      "2042/2595, train_loss: 0.5111\n",
      "2043/2595, train_loss: 0.1866\n",
      "2044/2595, train_loss: 0.2329\n",
      "2045/2595, train_loss: 0.1405\n",
      "2046/2595, train_loss: 0.1888\n",
      "2047/2595, train_loss: 0.4608\n",
      "2048/2595, train_loss: 0.6252\n",
      "2049/2595, train_loss: 0.4868\n",
      "2050/2595, train_loss: 0.4846\n",
      "2051/2595, train_loss: 0.3190\n",
      "2052/2595, train_loss: 0.2710\n",
      "2053/2595, train_loss: 0.2471\n",
      "2054/2595, train_loss: 0.4539\n",
      "2055/2595, train_loss: 0.8775\n",
      "2056/2595, train_loss: 0.6167\n",
      "2057/2595, train_loss: 0.6283\n",
      "2058/2595, train_loss: 0.2396\n",
      "2059/2595, train_loss: 0.3028\n",
      "2060/2595, train_loss: 0.2406\n",
      "2061/2595, train_loss: 0.3348\n",
      "2062/2595, train_loss: 0.4796\n",
      "2063/2595, train_loss: 0.7565\n",
      "2064/2595, train_loss: 0.2618\n",
      "2065/2595, train_loss: 0.3033\n",
      "2066/2595, train_loss: 0.3039\n",
      "2067/2595, train_loss: 0.2301\n",
      "2068/2595, train_loss: 0.2945\n",
      "2069/2595, train_loss: 0.3711\n",
      "2070/2595, train_loss: 0.5219\n",
      "2071/2595, train_loss: 0.4653\n",
      "2072/2595, train_loss: 0.4206\n",
      "2073/2595, train_loss: 0.1136\n",
      "2074/2595, train_loss: 0.3275\n",
      "2075/2595, train_loss: 0.7648\n",
      "2076/2595, train_loss: 0.2943\n",
      "2077/2595, train_loss: 0.4576\n",
      "2078/2595, train_loss: 0.3281\n",
      "2079/2595, train_loss: 0.1886\n",
      "2080/2595, train_loss: 0.2017\n",
      "2081/2595, train_loss: 0.3589\n",
      "2082/2595, train_loss: 0.1873\n",
      "2083/2595, train_loss: 0.3217\n",
      "2084/2595, train_loss: 0.4747\n",
      "2085/2595, train_loss: 0.3051\n",
      "2086/2595, train_loss: 0.3818\n",
      "2087/2595, train_loss: 0.1810\n",
      "2088/2595, train_loss: 0.3654\n",
      "2089/2595, train_loss: 0.2117\n",
      "2090/2595, train_loss: 0.2562\n",
      "2091/2595, train_loss: 0.1503\n",
      "2092/2595, train_loss: 0.2672\n",
      "2093/2595, train_loss: 0.2605\n",
      "2094/2595, train_loss: 0.1538\n",
      "2095/2595, train_loss: 0.3048\n",
      "2096/2595, train_loss: 0.0624\n",
      "2097/2595, train_loss: 0.0667\n",
      "2098/2595, train_loss: 0.0528\n",
      "2099/2595, train_loss: 0.5164\n",
      "2100/2595, train_loss: 0.4089\n",
      "2101/2595, train_loss: 0.1109\n",
      "2102/2595, train_loss: 0.4697\n",
      "2103/2595, train_loss: 0.2740\n",
      "2104/2595, train_loss: 0.5548\n",
      "2105/2595, train_loss: 0.1851\n",
      "2106/2595, train_loss: 0.2036\n",
      "2107/2595, train_loss: 0.5557\n",
      "2108/2595, train_loss: 0.4576\n",
      "2109/2595, train_loss: 0.1955\n",
      "2110/2595, train_loss: 0.0856\n",
      "2111/2595, train_loss: 0.1997\n",
      "2112/2595, train_loss: 0.3986\n",
      "2113/2595, train_loss: 0.1737\n",
      "2114/2595, train_loss: 0.2349\n",
      "2115/2595, train_loss: 0.2898\n",
      "2116/2595, train_loss: 0.3109\n",
      "2117/2595, train_loss: 0.2883\n",
      "2118/2595, train_loss: 0.1021\n",
      "2119/2595, train_loss: 1.2542\n",
      "2120/2595, train_loss: 0.3377\n",
      "2121/2595, train_loss: 0.0611\n",
      "2122/2595, train_loss: 0.4143\n",
      "2123/2595, train_loss: 0.5240\n",
      "2124/2595, train_loss: 0.3039\n",
      "2125/2595, train_loss: 0.3017\n",
      "2126/2595, train_loss: 0.1517\n",
      "2127/2595, train_loss: 0.3448\n",
      "2128/2595, train_loss: 1.0117\n",
      "2129/2595, train_loss: 0.1676\n",
      "2130/2595, train_loss: 0.1644\n",
      "2131/2595, train_loss: 0.1490\n",
      "2132/2595, train_loss: 0.2180\n",
      "2133/2595, train_loss: 0.2922\n",
      "2134/2595, train_loss: 0.6476\n",
      "2135/2595, train_loss: 0.2162\n",
      "2136/2595, train_loss: 0.1302\n",
      "2137/2595, train_loss: 0.3235\n",
      "2138/2595, train_loss: 0.3574\n",
      "2139/2595, train_loss: 0.2389\n",
      "2140/2595, train_loss: 0.1813\n",
      "2141/2595, train_loss: 0.1394\n",
      "2142/2595, train_loss: 0.2241\n",
      "2143/2595, train_loss: 0.2612\n",
      "2144/2595, train_loss: 0.1957\n",
      "2145/2595, train_loss: 0.7833\n",
      "2146/2595, train_loss: 0.0840\n",
      "2147/2595, train_loss: 0.4381\n",
      "2148/2595, train_loss: 0.0800\n",
      "2149/2595, train_loss: 0.3297\n",
      "2150/2595, train_loss: 0.1203\n",
      "2151/2595, train_loss: 0.5491\n",
      "2152/2595, train_loss: 0.6950\n",
      "2153/2595, train_loss: 0.3682\n",
      "2154/2595, train_loss: 0.3598\n",
      "2155/2595, train_loss: 0.0631\n",
      "2156/2595, train_loss: 0.3237\n",
      "2157/2595, train_loss: 0.1105\n",
      "2158/2595, train_loss: 0.3765\n",
      "2159/2595, train_loss: 0.5250\n",
      "2160/2595, train_loss: 0.2845\n",
      "2161/2595, train_loss: 0.2094\n",
      "2162/2595, train_loss: 0.2391\n",
      "2163/2595, train_loss: 0.3184\n",
      "2164/2595, train_loss: 0.4418\n",
      "2165/2595, train_loss: 0.0739\n",
      "2166/2595, train_loss: 0.1436\n",
      "2167/2595, train_loss: 0.1777\n",
      "2168/2595, train_loss: 0.4746\n",
      "2169/2595, train_loss: 0.0955\n",
      "2170/2595, train_loss: 0.8328\n",
      "2171/2595, train_loss: 0.2756\n",
      "2172/2595, train_loss: 0.0959\n",
      "2173/2595, train_loss: 0.3521\n",
      "2174/2595, train_loss: 0.1933\n",
      "2175/2595, train_loss: 0.3526\n",
      "2176/2595, train_loss: 0.2382\n",
      "2177/2595, train_loss: 0.1893\n",
      "2178/2595, train_loss: 0.1229\n",
      "2179/2595, train_loss: 0.3412\n",
      "2180/2595, train_loss: 0.8213\n",
      "2181/2595, train_loss: 0.3123\n",
      "2182/2595, train_loss: 0.7229\n",
      "2183/2595, train_loss: 0.2882\n",
      "2184/2595, train_loss: 0.3148\n",
      "2185/2595, train_loss: 0.4191\n",
      "2186/2595, train_loss: 0.6042\n",
      "2187/2595, train_loss: 0.1318\n",
      "2188/2595, train_loss: 0.1726\n",
      "2189/2595, train_loss: 0.4757\n",
      "2190/2595, train_loss: 0.1871\n",
      "2191/2595, train_loss: 0.4068\n",
      "2192/2595, train_loss: 0.4136\n",
      "2193/2595, train_loss: 0.1917\n",
      "2194/2595, train_loss: 0.6305\n",
      "2195/2595, train_loss: 0.5383\n",
      "2196/2595, train_loss: 0.6497\n",
      "2197/2595, train_loss: 0.2506\n",
      "2198/2595, train_loss: 0.6870\n",
      "2199/2595, train_loss: 0.3904\n",
      "2200/2595, train_loss: 0.8305\n",
      "2201/2595, train_loss: 0.1823\n",
      "2202/2595, train_loss: 0.3328\n",
      "2203/2595, train_loss: 0.2037\n",
      "2204/2595, train_loss: 0.2990\n",
      "2205/2595, train_loss: 0.1473\n",
      "2206/2595, train_loss: 0.2157\n",
      "2207/2595, train_loss: 0.2769\n",
      "2208/2595, train_loss: 0.4505\n",
      "2209/2595, train_loss: 0.2393\n",
      "2210/2595, train_loss: 0.1811\n",
      "2211/2595, train_loss: 0.3152\n",
      "2212/2595, train_loss: 0.2188\n",
      "2213/2595, train_loss: 0.5855\n",
      "2214/2595, train_loss: 0.1321\n",
      "2215/2595, train_loss: 0.1757\n",
      "2216/2595, train_loss: 0.2250\n",
      "2217/2595, train_loss: 0.2767\n",
      "2218/2595, train_loss: 0.1330\n",
      "2219/2595, train_loss: 0.2316\n",
      "2220/2595, train_loss: 0.1171\n",
      "2221/2595, train_loss: 0.1288\n",
      "2222/2595, train_loss: 0.3618\n",
      "2223/2595, train_loss: 0.5316\n",
      "2224/2595, train_loss: 0.1025\n",
      "2225/2595, train_loss: 0.3736\n",
      "2226/2595, train_loss: 0.3362\n",
      "2227/2595, train_loss: 0.4251\n",
      "2228/2595, train_loss: 0.5752\n",
      "2229/2595, train_loss: 0.4000\n",
      "2230/2595, train_loss: 0.8058\n",
      "2231/2595, train_loss: 0.5003\n",
      "2232/2595, train_loss: 0.2882\n",
      "2233/2595, train_loss: 0.0997\n",
      "2234/2595, train_loss: 0.3975\n",
      "2235/2595, train_loss: 0.2328\n",
      "2236/2595, train_loss: 0.3344\n",
      "2237/2595, train_loss: 0.1840\n",
      "2238/2595, train_loss: 0.5019\n",
      "2239/2595, train_loss: 0.1878\n",
      "2240/2595, train_loss: 0.6214\n",
      "2241/2595, train_loss: 0.5223\n",
      "2242/2595, train_loss: 0.1157\n",
      "2243/2595, train_loss: 0.3089\n",
      "2244/2595, train_loss: 0.4617\n",
      "2245/2595, train_loss: 0.3593\n",
      "2246/2595, train_loss: 0.5447\n",
      "2247/2595, train_loss: 0.1512\n",
      "2248/2595, train_loss: 0.2255\n",
      "2249/2595, train_loss: 0.1369\n",
      "2250/2595, train_loss: 0.7592\n",
      "2251/2595, train_loss: 0.7572\n",
      "2252/2595, train_loss: 0.2208\n",
      "2253/2595, train_loss: 0.2792\n",
      "2254/2595, train_loss: 0.1864\n",
      "2255/2595, train_loss: 0.5263\n",
      "2256/2595, train_loss: 0.3527\n",
      "2257/2595, train_loss: 0.5639\n",
      "2258/2595, train_loss: 0.3120\n",
      "2259/2595, train_loss: 0.5512\n",
      "2260/2595, train_loss: 0.5760\n",
      "2261/2595, train_loss: 0.5741\n",
      "2262/2595, train_loss: 0.2361\n",
      "2263/2595, train_loss: 0.1235\n",
      "2264/2595, train_loss: 0.1896\n",
      "2265/2595, train_loss: 0.1522\n",
      "2266/2595, train_loss: 0.7586\n",
      "2267/2595, train_loss: 0.2345\n",
      "2268/2595, train_loss: 0.3436\n",
      "2269/2595, train_loss: 0.5268\n",
      "2270/2595, train_loss: 0.3553\n",
      "2271/2595, train_loss: 0.4182\n",
      "2272/2595, train_loss: 0.2446\n",
      "2273/2595, train_loss: 0.1897\n",
      "2274/2595, train_loss: 0.2795\n",
      "2275/2595, train_loss: 0.3881\n",
      "2276/2595, train_loss: 0.3696\n",
      "2277/2595, train_loss: 0.3475\n",
      "2278/2595, train_loss: 0.2125\n",
      "2279/2595, train_loss: 0.3777\n",
      "2280/2595, train_loss: 0.1341\n",
      "2281/2595, train_loss: 0.3718\n",
      "2282/2595, train_loss: 0.2203\n",
      "2283/2595, train_loss: 0.4588\n",
      "2284/2595, train_loss: 0.3159\n",
      "2285/2595, train_loss: 0.3956\n",
      "2286/2595, train_loss: 1.0744\n",
      "2287/2595, train_loss: 0.1541\n",
      "2288/2595, train_loss: 0.4485\n",
      "2289/2595, train_loss: 0.1531\n",
      "2290/2595, train_loss: 0.1442\n",
      "2291/2595, train_loss: 0.2894\n",
      "2292/2595, train_loss: 0.2966\n",
      "2293/2595, train_loss: 0.2176\n",
      "2294/2595, train_loss: 0.2567\n",
      "2295/2595, train_loss: 0.1982\n",
      "2296/2595, train_loss: 0.4106\n",
      "2297/2595, train_loss: 0.4073\n",
      "2298/2595, train_loss: 0.2808\n",
      "2299/2595, train_loss: 0.1526\n",
      "2300/2595, train_loss: 0.0811\n",
      "2301/2595, train_loss: 0.4457\n",
      "2302/2595, train_loss: 0.9755\n",
      "2303/2595, train_loss: 0.1668\n",
      "2304/2595, train_loss: 0.3374\n",
      "2305/2595, train_loss: 0.5380\n",
      "2306/2595, train_loss: 0.1648\n",
      "2307/2595, train_loss: 0.2175\n",
      "2308/2595, train_loss: 0.2515\n",
      "2309/2595, train_loss: 0.0873\n",
      "2310/2595, train_loss: 0.1902\n",
      "2311/2595, train_loss: 0.2601\n",
      "2312/2595, train_loss: 0.4883\n",
      "2313/2595, train_loss: 0.2484\n",
      "2314/2595, train_loss: 0.1357\n",
      "2315/2595, train_loss: 0.5434\n",
      "2316/2595, train_loss: 0.2152\n",
      "2317/2595, train_loss: 0.2453\n",
      "2318/2595, train_loss: 0.2323\n",
      "2319/2595, train_loss: 0.6651\n",
      "2320/2595, train_loss: 0.0714\n",
      "2321/2595, train_loss: 0.2325\n",
      "2322/2595, train_loss: 0.1492\n",
      "2323/2595, train_loss: 0.3293\n",
      "2324/2595, train_loss: 0.4400\n",
      "2325/2595, train_loss: 0.6502\n",
      "2326/2595, train_loss: 0.1307\n",
      "2327/2595, train_loss: 0.1214\n",
      "2328/2595, train_loss: 0.2205\n",
      "2329/2595, train_loss: 0.3516\n",
      "2330/2595, train_loss: 0.0707\n",
      "2331/2595, train_loss: 0.2056\n",
      "2332/2595, train_loss: 0.2220\n",
      "2333/2595, train_loss: 0.3291\n",
      "2334/2595, train_loss: 0.2921\n",
      "2335/2595, train_loss: 0.1120\n",
      "2336/2595, train_loss: 0.2734\n",
      "2337/2595, train_loss: 0.1880\n",
      "2338/2595, train_loss: 0.1716\n",
      "2339/2595, train_loss: 0.3803\n",
      "2340/2595, train_loss: 0.1510\n",
      "2341/2595, train_loss: 0.7763\n",
      "2342/2595, train_loss: 0.6151\n",
      "2343/2595, train_loss: 0.4890\n",
      "2344/2595, train_loss: 0.2830\n",
      "2345/2595, train_loss: 0.2495\n",
      "2346/2595, train_loss: 0.1941\n",
      "2347/2595, train_loss: 0.1870\n",
      "2348/2595, train_loss: 0.4663\n",
      "2349/2595, train_loss: 0.2573\n",
      "2350/2595, train_loss: 0.1609\n",
      "2351/2595, train_loss: 0.1268\n",
      "2352/2595, train_loss: 0.3206\n",
      "2353/2595, train_loss: 0.1038\n",
      "2354/2595, train_loss: 0.2854\n",
      "2355/2595, train_loss: 0.2380\n",
      "2356/2595, train_loss: 0.5028\n",
      "2357/2595, train_loss: 0.1474\n",
      "2358/2595, train_loss: 1.0196\n",
      "2359/2595, train_loss: 0.1255\n",
      "2360/2595, train_loss: 0.4385\n",
      "2361/2595, train_loss: 0.5729\n",
      "2362/2595, train_loss: 0.1959\n",
      "2363/2595, train_loss: 0.3824\n",
      "2364/2595, train_loss: 0.6160\n",
      "2365/2595, train_loss: 0.2512\n",
      "2366/2595, train_loss: 0.2806\n",
      "2367/2595, train_loss: 0.4934\n",
      "2368/2595, train_loss: 0.2273\n",
      "2369/2595, train_loss: 0.4342\n",
      "2370/2595, train_loss: 0.1317\n",
      "2371/2595, train_loss: 0.3002\n",
      "2372/2595, train_loss: 0.1135\n",
      "2373/2595, train_loss: 0.1602\n",
      "2374/2595, train_loss: 0.1694\n",
      "2375/2595, train_loss: 0.2027\n",
      "2376/2595, train_loss: 0.1756\n",
      "2377/2595, train_loss: 0.5405\n",
      "2378/2595, train_loss: 0.0967\n",
      "2379/2595, train_loss: 0.7475\n",
      "2380/2595, train_loss: 0.0839\n",
      "2381/2595, train_loss: 0.2304\n",
      "2382/2595, train_loss: 0.2410\n",
      "2383/2595, train_loss: 0.3770\n",
      "2384/2595, train_loss: 0.2023\n",
      "2385/2595, train_loss: 0.3775\n",
      "2386/2595, train_loss: 0.6930\n",
      "2387/2595, train_loss: 0.1685\n",
      "2388/2595, train_loss: 0.2004\n",
      "2389/2595, train_loss: 0.0691\n",
      "2390/2595, train_loss: 0.0632\n",
      "2391/2595, train_loss: 0.1717\n",
      "2392/2595, train_loss: 0.4116\n",
      "2393/2595, train_loss: 0.0774\n",
      "2394/2595, train_loss: 0.0512\n",
      "2395/2595, train_loss: 0.2612\n",
      "2396/2595, train_loss: 0.7965\n",
      "2397/2595, train_loss: 0.0674\n",
      "2398/2595, train_loss: 0.0926\n",
      "2399/2595, train_loss: 0.3703\n",
      "2400/2595, train_loss: 0.2443\n",
      "2401/2595, train_loss: 0.1815\n",
      "2402/2595, train_loss: 0.1614\n",
      "2403/2595, train_loss: 0.7999\n",
      "2404/2595, train_loss: 0.2189\n",
      "2405/2595, train_loss: 0.4356\n",
      "2406/2595, train_loss: 0.3170\n",
      "2407/2595, train_loss: 0.2750\n",
      "2408/2595, train_loss: 0.6375\n",
      "2409/2595, train_loss: 0.4455\n",
      "2410/2595, train_loss: 0.1840\n",
      "2411/2595, train_loss: 0.3509\n",
      "2412/2595, train_loss: 0.0895\n",
      "2413/2595, train_loss: 0.2549\n",
      "2414/2595, train_loss: 0.1804\n",
      "2415/2595, train_loss: 0.2062\n",
      "2416/2595, train_loss: 0.3153\n",
      "2417/2595, train_loss: 0.4720\n",
      "2418/2595, train_loss: 0.3179\n",
      "2419/2595, train_loss: 0.1063\n",
      "2420/2595, train_loss: 0.5614\n",
      "2421/2595, train_loss: 0.3831\n",
      "2422/2595, train_loss: 0.1022\n",
      "2423/2595, train_loss: 0.1651\n",
      "2424/2595, train_loss: 0.4429\n",
      "2425/2595, train_loss: 0.1555\n",
      "2426/2595, train_loss: 0.3691\n",
      "2427/2595, train_loss: 0.1148\n",
      "2428/2595, train_loss: 0.1435\n",
      "2429/2595, train_loss: 0.1393\n",
      "2430/2595, train_loss: 0.2932\n",
      "2431/2595, train_loss: 0.1188\n",
      "2432/2595, train_loss: 0.6794\n",
      "2433/2595, train_loss: 0.5333\n",
      "2434/2595, train_loss: 0.6034\n",
      "2435/2595, train_loss: 0.2098\n",
      "2436/2595, train_loss: 0.1013\n",
      "2437/2595, train_loss: 0.2247\n",
      "2438/2595, train_loss: 0.6413\n",
      "2439/2595, train_loss: 0.2576\n",
      "2440/2595, train_loss: 0.6148\n",
      "2441/2595, train_loss: 0.1803\n",
      "2442/2595, train_loss: 0.6300\n",
      "2443/2595, train_loss: 0.1643\n",
      "2444/2595, train_loss: 0.5404\n",
      "2445/2595, train_loss: 0.2405\n",
      "2446/2595, train_loss: 0.1128\n",
      "2447/2595, train_loss: 0.5081\n",
      "2448/2595, train_loss: 0.6857\n",
      "2449/2595, train_loss: 0.2102\n",
      "2450/2595, train_loss: 0.1980\n",
      "2451/2595, train_loss: 0.7464\n",
      "2452/2595, train_loss: 0.2545\n",
      "2453/2595, train_loss: 0.3913\n",
      "2454/2595, train_loss: 0.0979\n",
      "2455/2595, train_loss: 0.1841\n",
      "2456/2595, train_loss: 0.2943\n",
      "2457/2595, train_loss: 0.2030\n",
      "2458/2595, train_loss: 0.2658\n",
      "2459/2595, train_loss: 0.1189\n",
      "2460/2595, train_loss: 0.3611\n",
      "2461/2595, train_loss: 0.6274\n",
      "2462/2595, train_loss: 0.3925\n",
      "2463/2595, train_loss: 0.0531\n",
      "2464/2595, train_loss: 0.2829\n",
      "2465/2595, train_loss: 0.1644\n",
      "2466/2595, train_loss: 0.2883\n",
      "2467/2595, train_loss: 0.1846\n",
      "2468/2595, train_loss: 0.0653\n",
      "2469/2595, train_loss: 0.3505\n",
      "2470/2595, train_loss: 0.2092\n",
      "2471/2595, train_loss: 0.2632\n",
      "2472/2595, train_loss: 0.5043\n",
      "2473/2595, train_loss: 0.1228\n",
      "2474/2595, train_loss: 0.5379\n",
      "2475/2595, train_loss: 0.1885\n",
      "2476/2595, train_loss: 0.0996\n",
      "2477/2595, train_loss: 0.2523\n",
      "2478/2595, train_loss: 0.0992\n",
      "2479/2595, train_loss: 0.5880\n",
      "2480/2595, train_loss: 0.9025\n",
      "2481/2595, train_loss: 0.4225\n",
      "2482/2595, train_loss: 0.2598\n",
      "2483/2595, train_loss: 0.2271\n",
      "2484/2595, train_loss: 0.2676\n",
      "2485/2595, train_loss: 0.3373\n",
      "2486/2595, train_loss: 0.3958\n",
      "2487/2595, train_loss: 0.1773\n",
      "2488/2595, train_loss: 0.3109\n",
      "2489/2595, train_loss: 0.3637\n",
      "2490/2595, train_loss: 0.2942\n",
      "2491/2595, train_loss: 0.2210\n",
      "2492/2595, train_loss: 0.1378\n",
      "2493/2595, train_loss: 0.4479\n",
      "2494/2595, train_loss: 0.2629\n",
      "2495/2595, train_loss: 0.4414\n",
      "2496/2595, train_loss: 0.1839\n",
      "2497/2595, train_loss: 0.0583\n",
      "2498/2595, train_loss: 0.2613\n",
      "2499/2595, train_loss: 0.1342\n",
      "2500/2595, train_loss: 0.3979\n",
      "2501/2595, train_loss: 0.1380\n",
      "2502/2595, train_loss: 0.0574\n",
      "2503/2595, train_loss: 0.5997\n",
      "2504/2595, train_loss: 0.5626\n",
      "2505/2595, train_loss: 0.4722\n",
      "2506/2595, train_loss: 0.1200\n",
      "2507/2595, train_loss: 0.3455\n",
      "2508/2595, train_loss: 0.2838\n",
      "2509/2595, train_loss: 0.0912\n",
      "2510/2595, train_loss: 0.1570\n",
      "2511/2595, train_loss: 0.1706\n",
      "2512/2595, train_loss: 0.0767\n",
      "2513/2595, train_loss: 0.1223\n",
      "2514/2595, train_loss: 0.1258\n",
      "2515/2595, train_loss: 0.0626\n",
      "2516/2595, train_loss: 0.6277\n",
      "2517/2595, train_loss: 0.7549\n",
      "2518/2595, train_loss: 0.1033\n",
      "2519/2595, train_loss: 0.6628\n",
      "2520/2595, train_loss: 0.4654\n",
      "2521/2595, train_loss: 0.2727\n",
      "2522/2595, train_loss: 0.0798\n",
      "2523/2595, train_loss: 0.2722\n",
      "2524/2595, train_loss: 0.3208\n",
      "2525/2595, train_loss: 0.2511\n",
      "2526/2595, train_loss: 0.2833\n",
      "2527/2595, train_loss: 0.1666\n",
      "2528/2595, train_loss: 0.2251\n",
      "2529/2595, train_loss: 0.3920\n",
      "2530/2595, train_loss: 0.7988\n",
      "2531/2595, train_loss: 0.1864\n",
      "2532/2595, train_loss: 0.1323\n",
      "2533/2595, train_loss: 0.2216\n",
      "2534/2595, train_loss: 0.1852\n",
      "2535/2595, train_loss: 0.0928\n",
      "2536/2595, train_loss: 0.2659\n",
      "2537/2595, train_loss: 0.3185\n",
      "2538/2595, train_loss: 0.3528\n",
      "2539/2595, train_loss: 0.1934\n",
      "2540/2595, train_loss: 0.1829\n",
      "2541/2595, train_loss: 0.2360\n",
      "2542/2595, train_loss: 0.2361\n",
      "2543/2595, train_loss: 0.3282\n",
      "2544/2595, train_loss: 0.4852\n",
      "2545/2595, train_loss: 0.5979\n",
      "2546/2595, train_loss: 0.1190\n",
      "2547/2595, train_loss: 0.2402\n",
      "2548/2595, train_loss: 0.2558\n",
      "2549/2595, train_loss: 0.0675\n",
      "2550/2595, train_loss: 0.2528\n",
      "2551/2595, train_loss: 0.4364\n",
      "2552/2595, train_loss: 0.2657\n",
      "2553/2595, train_loss: 0.0907\n",
      "2554/2595, train_loss: 0.5017\n",
      "2555/2595, train_loss: 0.4813\n",
      "2556/2595, train_loss: 0.2115\n",
      "2557/2595, train_loss: 0.5287\n",
      "2558/2595, train_loss: 0.0871\n",
      "2559/2595, train_loss: 0.2561\n",
      "2560/2595, train_loss: 0.2786\n",
      "2561/2595, train_loss: 0.1733\n",
      "2562/2595, train_loss: 0.3116\n",
      "2563/2595, train_loss: 0.4402\n",
      "2564/2595, train_loss: 0.3237\n",
      "2565/2595, train_loss: 0.9749\n",
      "2566/2595, train_loss: 0.0929\n",
      "2567/2595, train_loss: 0.2139\n",
      "2568/2595, train_loss: 0.3886\n",
      "2569/2595, train_loss: 0.2502\n",
      "2570/2595, train_loss: 0.7313\n",
      "2571/2595, train_loss: 0.3024\n",
      "2572/2595, train_loss: 0.2330\n",
      "2573/2595, train_loss: 0.0906\n",
      "2574/2595, train_loss: 0.5588\n",
      "2575/2595, train_loss: 0.4955\n",
      "2576/2595, train_loss: 0.2741\n",
      "2577/2595, train_loss: 0.3417\n",
      "2578/2595, train_loss: 0.3002\n",
      "2579/2595, train_loss: 0.2567\n",
      "2580/2595, train_loss: 0.2192\n",
      "2581/2595, train_loss: 0.2414\n",
      "2582/2595, train_loss: 0.1721\n",
      "2583/2595, train_loss: 0.5349\n",
      "2584/2595, train_loss: 0.2901\n",
      "2585/2595, train_loss: 0.5380\n",
      "2586/2595, train_loss: 0.1592\n",
      "2587/2595, train_loss: 0.5004\n",
      "2588/2595, train_loss: 0.4511\n",
      "2589/2595, train_loss: 0.0833\n",
      "2590/2595, train_loss: 0.3189\n",
      "2591/2595, train_loss: 0.2279\n",
      "2592/2595, train_loss: 0.2761\n",
      "2593/2595, train_loss: 0.2910\n",
      "2594/2595, train_loss: 0.1471\n",
      "2595/2595, train_loss: 0.3836\n",
      "2596/2595, train_loss: 0.2664\n",
      "epoch 1 average loss: 0.3614\n",
      "saved new best metric model\n",
      "current epoch: 1 current AUC: 0.9646 current accuracy: 0.9054 best AUC: 0.9646 at epoch: 1\n",
      "----------\n",
      "epoch 2/6\n",
      "1/2595, train_loss: 0.1473\n",
      "2/2595, train_loss: 0.1302\n",
      "3/2595, train_loss: 0.2280\n",
      "4/2595, train_loss: 0.4568\n",
      "5/2595, train_loss: 0.1681\n",
      "6/2595, train_loss: 0.3337\n",
      "7/2595, train_loss: 0.3303\n",
      "8/2595, train_loss: 0.2014\n",
      "9/2595, train_loss: 0.0618\n",
      "10/2595, train_loss: 0.1240\n",
      "11/2595, train_loss: 0.3394\n",
      "12/2595, train_loss: 0.0773\n",
      "13/2595, train_loss: 0.6113\n",
      "14/2595, train_loss: 0.1311\n",
      "15/2595, train_loss: 0.0788\n",
      "16/2595, train_loss: 0.2096\n",
      "17/2595, train_loss: 0.2335\n",
      "18/2595, train_loss: 0.0999\n",
      "19/2595, train_loss: 0.5442\n",
      "20/2595, train_loss: 0.3139\n",
      "21/2595, train_loss: 0.2771\n",
      "22/2595, train_loss: 0.5756\n",
      "23/2595, train_loss: 0.3527\n",
      "24/2595, train_loss: 0.4887\n",
      "25/2595, train_loss: 0.9113\n",
      "26/2595, train_loss: 0.4241\n",
      "27/2595, train_loss: 0.1127\n",
      "28/2595, train_loss: 0.3162\n",
      "29/2595, train_loss: 0.3461\n",
      "30/2595, train_loss: 0.3153\n",
      "31/2595, train_loss: 0.3152\n",
      "32/2595, train_loss: 0.3425\n",
      "33/2595, train_loss: 0.5098\n",
      "34/2595, train_loss: 0.1519\n",
      "35/2595, train_loss: 0.5082\n",
      "36/2595, train_loss: 0.2432\n",
      "37/2595, train_loss: 0.2499\n",
      "38/2595, train_loss: 0.1991\n",
      "39/2595, train_loss: 0.2357\n",
      "40/2595, train_loss: 0.1747\n",
      "41/2595, train_loss: 0.2367\n",
      "42/2595, train_loss: 0.4819\n",
      "43/2595, train_loss: 0.2243\n",
      "44/2595, train_loss: 0.2974\n",
      "45/2595, train_loss: 0.1882\n",
      "46/2595, train_loss: 0.7291\n",
      "47/2595, train_loss: 0.6474\n",
      "48/2595, train_loss: 0.2715\n",
      "49/2595, train_loss: 0.1702\n",
      "50/2595, train_loss: 0.5652\n",
      "51/2595, train_loss: 0.1477\n",
      "52/2595, train_loss: 0.2152\n",
      "53/2595, train_loss: 0.0803\n",
      "54/2595, train_loss: 0.1446\n",
      "55/2595, train_loss: 0.1338\n",
      "56/2595, train_loss: 0.1371\n",
      "57/2595, train_loss: 0.2746\n",
      "58/2595, train_loss: 0.1961\n",
      "59/2595, train_loss: 0.3681\n",
      "60/2595, train_loss: 0.0907\n",
      "61/2595, train_loss: 0.6988\n",
      "62/2595, train_loss: 0.4252\n",
      "63/2595, train_loss: 0.2490\n",
      "64/2595, train_loss: 0.2773\n",
      "65/2595, train_loss: 0.1988\n",
      "66/2595, train_loss: 0.1940\n",
      "67/2595, train_loss: 0.0751\n",
      "68/2595, train_loss: 0.1062\n",
      "69/2595, train_loss: 0.2037\n",
      "70/2595, train_loss: 0.3604\n",
      "71/2595, train_loss: 0.0699\n",
      "72/2595, train_loss: 0.1565\n",
      "73/2595, train_loss: 0.5354\n",
      "74/2595, train_loss: 0.6727\n",
      "75/2595, train_loss: 0.8062\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "학습 결과 확인"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "plt.figure(\"train\", (12, 6)) #\r\n",
    "plt.subplot(1, 2, 1)\r\n",
    "plt.title(\"Epoch Average Loss\")\r\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))] #0부터 eppch_loss가 계산된 횟수까지 리스트에 저장\r\n",
    "y = epoch_loss_values #epoch_loss값\r\n",
    "plt.xlabel(\"epoch\") #축은 epoch\r\n",
    "plt.plot(x, y) \r\n",
    "plt.subplot(1, 2, 2)\r\n",
    "plt.title(\"Val AUC\")\r\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))] #0에서 epoch수까지 1을 더해서 가져와서 validation interval과 곱함\r\n",
    "y = metric_values #epoch별 AUC값\r\n",
    "plt.xlabel(\"epoch\") #축은 epoch\r\n",
    "plt.plot(x, y)\r\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ],
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"386.034375pt\" version=\"1.1\" viewBox=\"0 0 713.265625 386.034375\" width=\"713.265625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-08-18T12:29:18.631504</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 386.034375 \r\nL 713.265625 386.034375 \r\nL 713.265625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 348.478125 \r\nL 340.829261 348.478125 \r\nL 340.829261 22.318125 \r\nL 36.465625 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"ma3f43cb082\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.300336\" xlink:href=\"#ma3f43cb082\" y=\"348.478125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(42.348773 363.076562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n        <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"96.416038\" xlink:href=\"#ma3f43cb082\" y=\"348.478125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 1.5 -->\r\n      <g transform=\"translate(88.464476 363.076562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"142.531741\" xlink:href=\"#ma3f43cb082\" y=\"348.478125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 2.0 -->\r\n      <g transform=\"translate(134.580178 363.076562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"188.647443\" xlink:href=\"#ma3f43cb082\" y=\"348.478125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 2.5 -->\r\n      <g transform=\"translate(180.695881 363.076562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"234.763146\" xlink:href=\"#ma3f43cb082\" y=\"348.478125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 3.0 -->\r\n      <g transform=\"translate(226.811583 363.076562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"280.878848\" xlink:href=\"#ma3f43cb082\" y=\"348.478125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 3.5 -->\r\n      <g transform=\"translate(272.927286 363.076562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"326.994551\" xlink:href=\"#ma3f43cb082\" y=\"348.478125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 4.0 -->\r\n      <g transform=\"translate(319.042988 363.076562)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_8\">\r\n     <!-- epoch -->\r\n     <g transform=\"translate(173.419318 376.754687)scale(0.1 -0.1)\">\r\n      <defs>\r\n       <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       <path d=\"M 18.109375 8.203125 \r\nL 18.109375 -20.796875 \r\nL 9.078125 -20.796875 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nz\r\nM 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-112\"/>\r\n       <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n       <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-104\"/>\r\n      </defs>\r\n      <use xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m558eba7f68\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m558eba7f68\" y=\"311.296039\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.22 -->\r\n      <g transform=\"translate(7.2 315.095258)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m558eba7f68\" y=\"269.295151\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.24 -->\r\n      <g transform=\"translate(7.2 273.09437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m558eba7f68\" y=\"227.294263\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.26 -->\r\n      <g transform=\"translate(7.2 231.093482)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m558eba7f68\" y=\"185.293375\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.28 -->\r\n      <g transform=\"translate(7.2 189.092593)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m558eba7f68\" y=\"143.292486\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.30 -->\r\n      <g transform=\"translate(7.2 147.091705)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m558eba7f68\" y=\"101.291598\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.32 -->\r\n      <g transform=\"translate(7.2 105.090817)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m558eba7f68\" y=\"59.29071\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 0.34 -->\r\n      <g transform=\"translate(7.2 63.089929)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_15\">\r\n    <path clip-path=\"url(#p89567f7ca1)\" d=\"M 50.300336 37.14358 \r\nL 142.531741 196.54459 \r\nL 234.763146 278.787865 \r\nL 326.994551 333.65267 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 348.478125 \r\nL 36.465625 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 340.829261 348.478125 \r\nL 340.829261 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 348.478125 \r\nL 340.829261 348.478125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 22.318125 \r\nL 340.829261 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_16\">\r\n    <!-- Epoch Average Loss -->\r\n    <g transform=\"translate(128.660568 16.318125)scale(0.12 -0.12)\">\r\n     <defs>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 55.90625 72.90625 \r\nL 55.90625 64.59375 \r\nL 19.671875 64.59375 \r\nL 19.671875 43.015625 \r\nL 54.390625 43.015625 \r\nL 54.390625 34.71875 \r\nL 19.671875 34.71875 \r\nL 19.671875 8.296875 \r\nL 56.78125 8.296875 \r\nL 56.78125 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-69\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n      <path d=\"M 34.1875 63.1875 \r\nL 20.796875 26.90625 \r\nL 47.609375 26.90625 \r\nz\r\nM 28.609375 72.90625 \r\nL 39.796875 72.90625 \r\nL 67.578125 0 \r\nL 57.328125 0 \r\nL 50.6875 18.703125 \r\nL 17.828125 18.703125 \r\nL 11.1875 0 \r\nL 0.78125 0 \r\nz\r\n\" id=\"DejaVuSans-65\"/>\r\n      <path d=\"M 2.984375 54.6875 \r\nL 12.5 54.6875 \r\nL 29.59375 8.796875 \r\nL 46.6875 54.6875 \r\nL 56.203125 54.6875 \r\nL 35.6875 0 \r\nL 23.484375 0 \r\nz\r\n\" id=\"DejaVuSans-118\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n      <path d=\"M 9.8125 72.90625 \r\nL 19.671875 72.90625 \r\nL 19.671875 8.296875 \r\nL 55.171875 8.296875 \r\nL 55.171875 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-76\"/>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-69\"/>\r\n     <use x=\"63.183594\" xlink:href=\"#DejaVuSans-112\"/>\r\n     <use x=\"126.660156\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"187.841797\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"242.822266\" xlink:href=\"#DejaVuSans-104\"/>\r\n     <use x=\"306.201172\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"337.988281\" xlink:href=\"#DejaVuSans-65\"/>\r\n     <use x=\"400.521484\" xlink:href=\"#DejaVuSans-118\"/>\r\n     <use x=\"459.701172\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"521.224609\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"562.337891\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"623.617188\" xlink:href=\"#DejaVuSans-103\"/>\r\n     <use x=\"687.09375\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"748.617188\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"780.404297\" xlink:href=\"#DejaVuSans-76\"/>\r\n     <use x=\"834.367188\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"895.548828\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"947.648438\" xlink:href=\"#DejaVuSans-115\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n  <g id=\"axes_2\">\r\n   <g id=\"patch_7\">\r\n    <path d=\"M 401.701989 348.478125 \r\nL 706.065625 348.478125 \r\nL 706.065625 22.318125 \r\nL 401.701989 22.318125 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_3\">\r\n    <g id=\"xtick_8\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"415.536699\" xlink:href=\"#ma3f43cb082\" y=\"348.478125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_17\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(407.585137 363.076562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_9\">\r\n     <g id=\"line2d_17\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"461.652402\" xlink:href=\"#ma3f43cb082\" y=\"348.478125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_18\">\r\n      <!-- 1.5 -->\r\n      <g transform=\"translate(453.700839 363.076562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_10\">\r\n     <g id=\"line2d_18\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"507.768104\" xlink:href=\"#ma3f43cb082\" y=\"348.478125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_19\">\r\n      <!-- 2.0 -->\r\n      <g transform=\"translate(499.816542 363.076562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_11\">\r\n     <g id=\"line2d_19\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"553.883807\" xlink:href=\"#ma3f43cb082\" y=\"348.478125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_20\">\r\n      <!-- 2.5 -->\r\n      <g transform=\"translate(545.932244 363.076562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_12\">\r\n     <g id=\"line2d_20\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"599.999509\" xlink:href=\"#ma3f43cb082\" y=\"348.478125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_21\">\r\n      <!-- 3.0 -->\r\n      <g transform=\"translate(592.047947 363.076562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_13\">\r\n     <g id=\"line2d_21\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"646.115212\" xlink:href=\"#ma3f43cb082\" y=\"348.478125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_22\">\r\n      <!-- 3.5 -->\r\n      <g transform=\"translate(638.163649 363.076562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_14\">\r\n     <g id=\"line2d_22\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"692.230914\" xlink:href=\"#ma3f43cb082\" y=\"348.478125\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_23\">\r\n      <!-- 4.0 -->\r\n      <g transform=\"translate(684.279352 363.076562)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"text_24\">\r\n     <!-- epoch -->\r\n     <g transform=\"translate(538.655682 376.754687)scale(0.1 -0.1)\">\r\n      <use xlink:href=\"#DejaVuSans-101\"/>\r\n      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-112\"/>\r\n      <use x=\"125\" xlink:href=\"#DejaVuSans-111\"/>\r\n      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-99\"/>\r\n      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-104\"/>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_4\">\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_23\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"401.701989\" xlink:href=\"#m558eba7f68\" y=\"310.806118\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_25\">\r\n      <!-- 0.970 -->\r\n      <g transform=\"translate(366.073864 314.605337)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.984375 1.515625 \r\nL 10.984375 10.5 \r\nQ 14.703125 8.734375 18.5 7.8125 \r\nQ 22.3125 6.890625 25.984375 6.890625 \r\nQ 35.75 6.890625 40.890625 13.453125 \r\nQ 46.046875 20.015625 46.78125 33.40625 \r\nQ 43.953125 29.203125 39.59375 26.953125 \r\nQ 35.25 24.703125 29.984375 24.703125 \r\nQ 19.046875 24.703125 12.671875 31.3125 \r\nQ 6.296875 37.9375 6.296875 49.421875 \r\nQ 6.296875 60.640625 12.9375 67.421875 \r\nQ 19.578125 74.21875 30.609375 74.21875 \r\nQ 43.265625 74.21875 49.921875 64.515625 \r\nQ 56.59375 54.828125 56.59375 36.375 \r\nQ 56.59375 19.140625 48.40625 8.859375 \r\nQ 40.234375 -1.421875 26.421875 -1.421875 \r\nQ 22.703125 -1.421875 18.890625 -0.6875 \r\nQ 15.09375 0.046875 10.984375 1.515625 \r\nz\r\nM 30.609375 32.421875 \r\nQ 37.25 32.421875 41.125 36.953125 \r\nQ 45.015625 41.5 45.015625 49.421875 \r\nQ 45.015625 57.28125 41.125 61.84375 \r\nQ 37.25 66.40625 30.609375 66.40625 \r\nQ 23.96875 66.40625 20.09375 61.84375 \r\nQ 16.21875 57.28125 16.21875 49.421875 \r\nQ 16.21875 41.5 20.09375 36.953125 \r\nQ 23.96875 32.421875 30.609375 32.421875 \r\nz\r\n\" id=\"DejaVuSans-57\"/>\r\n        <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_9\">\r\n     <g id=\"line2d_24\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"401.701989\" xlink:href=\"#m558eba7f68\" y=\"272.645053\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_26\">\r\n      <!-- 0.972 -->\r\n      <g transform=\"translate(366.073864 276.444271)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_10\">\r\n     <g id=\"line2d_25\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"401.701989\" xlink:href=\"#m558eba7f68\" y=\"234.483988\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_27\">\r\n      <!-- 0.974 -->\r\n      <g transform=\"translate(366.073864 238.283206)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_11\">\r\n     <g id=\"line2d_26\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"401.701989\" xlink:href=\"#m558eba7f68\" y=\"196.322922\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_28\">\r\n      <!-- 0.976 -->\r\n      <g transform=\"translate(366.073864 200.122141)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_12\">\r\n     <g id=\"line2d_27\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"401.701989\" xlink:href=\"#m558eba7f68\" y=\"158.161857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_29\">\r\n      <!-- 0.978 -->\r\n      <g transform=\"translate(366.073864 161.961076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-55\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_13\">\r\n     <g id=\"line2d_28\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"401.701989\" xlink:href=\"#m558eba7f68\" y=\"120.000792\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_30\">\r\n      <!-- 0.980 -->\r\n      <g transform=\"translate(366.073864 123.800011)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_14\">\r\n     <g id=\"line2d_29\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"401.701989\" xlink:href=\"#m558eba7f68\" y=\"81.839727\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_31\">\r\n      <!-- 0.982 -->\r\n      <g transform=\"translate(366.073864 85.638946)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_15\">\r\n     <g id=\"line2d_30\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"401.701989\" xlink:href=\"#m558eba7f68\" y=\"43.678662\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_32\">\r\n      <!-- 0.984 -->\r\n      <g transform=\"translate(366.073864 47.477881)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-57\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_31\">\r\n    <path clip-path=\"url(#pafa5080fdf)\" d=\"M 415.536699 333.65267 \r\nL 507.768104 255.556051 \r\nL 599.999509 37.14358 \r\nL 692.230914 76.334993 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path d=\"M 401.701989 348.478125 \r\nL 401.701989 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path d=\"M 706.065625 348.478125 \r\nL 706.065625 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path d=\"M 401.701989 348.478125 \r\nL 706.065625 348.478125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path d=\"M 401.701989 22.318125 \r\nL 706.065625 22.318125 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"text_33\">\r\n    <!-- Val AUC -->\r\n    <g transform=\"translate(530.308494 16.318125)scale(0.12 -0.12)\">\r\n     <defs>\r\n      <path d=\"M 28.609375 0 \r\nL 0.78125 72.90625 \r\nL 11.078125 72.90625 \r\nL 34.1875 11.53125 \r\nL 57.328125 72.90625 \r\nL 67.578125 72.90625 \r\nL 39.796875 0 \r\nz\r\n\" id=\"DejaVuSans-86\"/>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 8.6875 72.90625 \r\nL 18.609375 72.90625 \r\nL 18.609375 28.609375 \r\nQ 18.609375 16.890625 22.84375 11.734375 \r\nQ 27.09375 6.59375 36.625 6.59375 \r\nQ 46.09375 6.59375 50.34375 11.734375 \r\nQ 54.59375 16.890625 54.59375 28.609375 \r\nL 54.59375 72.90625 \r\nL 64.5 72.90625 \r\nL 64.5 27.390625 \r\nQ 64.5 13.140625 57.4375 5.859375 \r\nQ 50.390625 -1.421875 36.625 -1.421875 \r\nQ 22.796875 -1.421875 15.734375 5.859375 \r\nQ 8.6875 13.140625 8.6875 27.390625 \r\nz\r\n\" id=\"DejaVuSans-85\"/>\r\n      <path d=\"M 64.40625 67.28125 \r\nL 64.40625 56.890625 \r\nQ 59.421875 61.53125 53.78125 63.8125 \r\nQ 48.140625 66.109375 41.796875 66.109375 \r\nQ 29.296875 66.109375 22.65625 58.46875 \r\nQ 16.015625 50.828125 16.015625 36.375 \r\nQ 16.015625 21.96875 22.65625 14.328125 \r\nQ 29.296875 6.6875 41.796875 6.6875 \r\nQ 48.140625 6.6875 53.78125 8.984375 \r\nQ 59.421875 11.28125 64.40625 15.921875 \r\nL 64.40625 5.609375 \r\nQ 59.234375 2.09375 53.4375 0.328125 \r\nQ 47.65625 -1.421875 41.21875 -1.421875 \r\nQ 24.65625 -1.421875 15.125 8.703125 \r\nQ 5.609375 18.84375 5.609375 36.375 \r\nQ 5.609375 53.953125 15.125 64.078125 \r\nQ 24.65625 74.21875 41.21875 74.21875 \r\nQ 47.75 74.21875 53.53125 72.484375 \r\nQ 59.328125 70.75 64.40625 67.28125 \r\nz\r\n\" id=\"DejaVuSans-67\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-86\"/>\r\n     <use x=\"60.658203\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"121.9375\" xlink:href=\"#DejaVuSans-108\"/>\r\n     <use x=\"149.720703\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"181.507812\" xlink:href=\"#DejaVuSans-65\"/>\r\n     <use x=\"249.916016\" xlink:href=\"#DejaVuSans-85\"/>\r\n     <use x=\"323.109375\" xlink:href=\"#DejaVuSans-67\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p89567f7ca1\">\r\n   <rect height=\"326.16\" width=\"304.363636\" x=\"36.465625\" y=\"22.318125\"/>\r\n  </clipPath>\r\n  <clipPath id=\"pafa5080fdf\">\r\n   <rect height=\"326.16\" width=\"304.363636\" x=\"401.701989\" y=\"22.318125\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "model.load_state_dict(torch.load(\r\n",
    "    os.path.join(dir, \"best_metric_model.pth\")))\r\n",
    "model.eval()\r\n",
    "y_true = []\r\n",
    "y_pred = []\r\n",
    "with torch.no_grad():\r\n",
    "    for test_data in test_loader:\r\n",
    "        test_images, test_labels = (  #test_image와 test_label을 가져옴\r\n",
    "            test_data[0].permute(0, 3, 1, 2).to(device),\r\n",
    "            test_data[1].to(device),\r\n",
    "        )\r\n",
    "        pred = model(test_images).argmax(dim=1) #차원에서 텐서의 최대값 인덱스를 반환합니다.\r\n",
    "        for i in range(len(pred)):\r\n",
    "            y_true.append(test_labels[i].item()) # 정답들을 저장\r\n",
    "            y_pred.append(pred[i].item())        # 모델이 예측한 값들을 저장"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "print(classification_report(\r\n",
    "    y_true, y_pred, target_names=class_names, digits=4))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      cancer     0.9385    0.9383    0.9384      4000\n",
      "      normal     0.9383    0.9385    0.9384      4000\n",
      "\n",
      "    accuracy                         0.9384      8000\n",
      "   macro avg     0.9384    0.9384    0.9384      8000\n",
      "weighted avg     0.9384    0.9384    0.9384      8000\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference sequnce"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "확인하기 위한 이미지 불러오기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "infer_img = loader(image[0])\r\n",
    "infer_mask = loader(mask[0])\r\n",
    "\r\n",
    "# 위에서 선언한 image 중 첫번째 이미지 사용"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "모델과 학습 결과를 불러와 적용"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.load_state_dict(torch.load(os.path.join(dir, \"best_metric_model.pth\")))\r\n",
    "model.eval()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "무색의 히트맵을 만들 이미지 생성"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "heat_map = np.zeros((height, width))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def softmax(a) :\r\n",
    "    y = []\r\n",
    "    c,n = a\r\n",
    "    exp_c = np.exp(c)\r\n",
    "    exp_n = np.exp(n)\r\n",
    "    sum_exp_a = exp_c + exp_n\r\n",
    "    C = exp_c / sum_exp_a\r\n",
    "    N = exp_n / sum_exp_a\r\n",
    "\r\n",
    "    y.append(C)\r\n",
    "    y.append(N)\r\n",
    "\r\n",
    "    y = np.array(y)\r\n",
    "\r\n",
    "    return y        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "color_range2 = []\r\n",
    "\r\n",
    "for j in range(0,(height - 200), 50):\r\n",
    "    for i in range(0,(width - 200), 50):\r\n",
    "        \r\n",
    "        patch2 = img[j:j+200, i:i+200, :] / 255\r\n",
    "        patch2 = torch.tensor(patch2.reshape((1,) + patch2.shape)).permute(0,3,1,2).to(device)\r\n",
    "\r\n",
    "        y_p2 = model(patch2)[0].cpu().detach().numpy()\r\n",
    "        soft_y2 = softmax(y_p2)\r\n",
    "\r\n",
    "        heat_map[j:j+200, i:i+200] = soft_y2[0]\r\n",
    "\r\n",
    "        color_range.append(soft_y2[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "원본 이미지와 히트맵을 겹쳐서 확인 함으로 성능 확인"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(16,8))\r\n",
    "\r\n",
    "plt.subplot(1,2,1)\r\n",
    "plt.imshow(img/255)\r\n",
    "plt.imshow(heat_map, cmap='jet', alpha = 0.6)\r\n",
    "plt.clim(min(color_range), max(color_range))\r\n",
    "plt.colorbar()\r\n",
    "\r\n",
    "plt.subplot(1,2,2)\r\n",
    "plt.imshow(label/255)\r\n",
    "\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Do not open"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "la = torch.tensor([0]).to(device)\r\n",
    "\r\n",
    "        loss = int(loss_function(y_p, la).item())\r\n",
    "\r\n",
    "        # print(loss)\r\n",
    "\r\n",
    "        # 0 2 4 6\r\n",
    "        if loss <= 0:\r\n",
    "            # 암으로 판단되는 셀을 희색으로 변경\r\n",
    "            hit_map[((height//h)*j):((height//h)*(j+1)),((width//w)*i):((width//w)*(i+1)), :] = [255,0,0,1]\r\n",
    "        elif (loss >= 2 and loss < 4):\r\n",
    "            hit_map[((height//h)*j):((height//h)*(j+1)),((width//w)*i):((width//w)*(i+1)), :] = [255,0,0,0.8]\r\n",
    "        elif (loss >= 4 and loss < 5):\r\n",
    "            hit_map[((height//h)*j):((height//h)*(j+1)),((width//w)*i):((width//w)*(i+1)), :] = [255,0,0,0.6]\r\n",
    "        else:\r\n",
    "            pass\r\n",
    "            # hit_map[((height//h)*j):((height//h)*(j+1)),((width//w)*i):((width//w)*(i+1)), :] = [0,0,0,1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "from PIL import Image\r\n",
    "\r\n",
    "test = Image.open('D:/python_study/patches/normal/img01/Image010001_normal.png')\r\n",
    "\r\n",
    "pix = np.array(test.getdata())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "pix.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(40000, 4)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit"
  },
  "interpreter": {
   "hash": "b6c98351b93b22dcfef8d5b66d292f4e5667cf42682fa1575599ea5c894db108"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}